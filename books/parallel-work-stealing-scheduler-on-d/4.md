---
title: 4 行列積の並列処理の評価
---
## 4 行列積の並列処理の評価

第4章では、行列積の処理速度の計測を通じて、第2章で実装したスケジューラを利用した場合の**台数効果**を確認する。
具体的には、正方行列 $A,B$ の積だが、行列 $B$ のキャッシュミスを抑制するため、行列 $B$ を転置した $AB^\top$ の形式とする。

```d
import core.atomic, std.algorithm, std.numeric, std.parallelism, std.random, std.range, std.stdio;
```

まず、行列の配列を宣言する。各行の末尾に空の領域を設定し、第3章で学んだフォルスシェアリングによる律速を防ぐ。
また、第2章で実装したスケジューラの変数も宣言する。実行時にプロセッサ数が決まるので、初期化は実行時に行う。

```d
const int N = 8192;
const int PAD = 32;
const int PADDED = N + PAD;
shared double[N * PADDED] A;
shared double[N * PADDED] B;
shared double[N * PADDED] C = 0;
shared Dawn!(int, int, int, int, int, int, int, int) sched;
```

次に、部分行列の積の逐次処理を実装する。1行1列に至るまで並列化すると効率が悪化するので、逐次処理を併用する。

```d
void dmm_leaf(int i1, int i2, int j1, int j2, int k1, int k2) {
  foreach(i,iN; enumerate(iota(i1 * PADDED, i2 * PADDED, PADDED), i1))
  foreach(j,jN; enumerate(iota(j1 * PADDED, j2 * PADDED, PADDED), j1))
    C[iN+j].atomicOp!"+="(A[iN+k1..iN+k2].dotProduct(B[jN+k1..jN+k2]));
}
```

### 4.1 提案実装による並列化

行列を再帰的に分割し、部分行列を計算するタスクを分岐して、分割統治法とワークスティーリングで並列処理を行う。
最長軸を分割し、部分行列が正方行列に近付くほど、同じ計算量でも参照が局所化され、キャッシュミスを抑制できる。

```d
int dmm_dawn(int i1, int i2, int j1, int j2, int k1, int k2, int grain) {
  auto axes = [i2 - i1, j2 - j1, k2 - k1];
  if(axes.maxElement <= grain) {
    dmm_leaf(i1, i2, j1, j2, k1, k2);
  } else if(axes.maxIndex == 0) {
    auto t1 = sched.fork!dmm_dawn(i1, (i1+i2)/2, j1, j2, k1, k2, grain);
    auto t2 = sched.fork!dmm_dawn((i1+i2)/2, i2, j1, j2, k1, k2, grain);
    sched.join(t1);
    sched.join(t2);
  } else if(axes.maxIndex == 1) {
    auto t1 = sched.fork!dmm_dawn(i1, i2, j1, (j1+j2)/2, k1, k2, grain);
    auto t2 = sched.fork!dmm_dawn(i1, i2, (j1+j2)/2, j2, k1, k2, grain);
    sched.join(t1);
    sched.join(t2);
  } else if(axes.maxIndex == 2) {
    auto t1 = sched.fork!dmm_dawn(i1, i2, j1, j2, k1, (k1+k2)/2, grain);
    auto t2 = sched.fork!dmm_dawn(i1, i2, j1, j2, (k1+k2)/2, k2, grain);
    sched.join(t1);
    sched.join(t2);
  }
  return 0;
}
```

### 4.2 既存実装による並列化

提案実装と比較するため、D言語の標準ライブラリに含まれるTaskPoolを利用して、同じ手順で行列積を並列化する。
提案実装との比較により、Fig. 1.1の集中管理型の並列処理と、Fig. 1.2の分散管理型の並列処理の、性能差を検証する。

```d
void dmm_pool(int i1, int i2, int j1, int j2, int k1, int k2, int grain) {
  auto axes = [i2 - i1, j2 - j1, k2 - k1];
  if(axes[axes.maxIndex] <= grain) {
    dmm_leaf(i1, i2, j1, j2, k1, k2);
  } else if(axes.maxIndex == 0) {
    auto t1 = task!dmm_pool(i1, (i1+i2)/2, j1, j2, k1, k2, grain);
    auto t2 = task!dmm_pool((i1+i2)/2, i2, j1, j2, k1, k2, grain);
    taskPool.put(t1);
    taskPool.put(t2);
    t1.workForce;
    t2.workForce;
  } else if(axes.maxIndex == 1) {
    auto t1 = task!dmm_pool(i1, i2, j1, (j1+j2)/2, k1, k2, grain);
    auto t2 = task!dmm_pool(i1, i2, (j1+j2)/2, j2, k1, k2, grain);
    taskPool.put(t1);
    taskPool.put(t2);
    t1.workForce;
    t2.workForce;
  } else if(axes.maxIndex == 2) {
    auto t1 = task!dmm_pool(i1, i2, j1, j2, k1, (k1+k2)/2, grain);
    auto t2 = task!dmm_pool(i1, i2, j1, j2, (k1+k2)/2, k2, grain);
    taskPool.put(t1);
    taskPool.put(t2);
    t1.workForce;
    t2.workForce;
  }
}
```

### 4.3 反復処理による並列化

再帰的な分割統治を実施せず、行列を3軸で末端粒度まで分割し、逐次的に暇なプロセッサに分配する方法を実装する。

```d
void dmm_gr3d(int i1, int i2, int j1, int j2, int k1, int k2, int grain) {
  const int iN = (i2 - i1) / grain;
  const int jN = (j2 - j1) / grain;
  const int kN = (k2 - k1) / grain;
  foreach(ch; parallel(iota(0, iN * jN * kN), 1)) {
    const int i = i1 + ch / kN / jN * grain;
    const int j = j1 + ch / kN % jN * grain;
    const int k = k1 + ch % kN * grain;
    dmm_leaf(i, i + grain, j, j + grain, k, k + grain);
  }
}
```

行列を2軸で分割した場合も実装する。 $k$ 軸で分割せず、 $i,j$ 軸を並列化した場合である。部分行列は $k$ 軸のみ長くなる。

```d
void dmm_gr2d(int i1, int i2, int j1, int j2, int k1, int k2, int grain) {
  const int iN = (i2 - i1) / grain;
  const int jN = (j2 - j1) / grain;
  foreach(ch; parallel(iota(0, iN * jN), 1)) {
    const int i = i1 + ch / jN * grain;
    const int j = j1 + ch % jN * grain;
    dmm_leaf(i, i + grain, j, j + grain, k1, k2);
  }
}
```

### 4.4 台数効果の評価と解釈

最後に、main関数を実装する。行列積に含まれる乗算と加算の回数を処理時間で割ると、処理速度のflops値が求まる。

```d
void main() {
  import std.datetime.stopwatch;
  defaultPoolThreads = totalCPUs;
  sched = new typeof(sched);
  foreach(i; iota(A.length)) A[i] = uniform(0, 1);
  foreach(i; iota(B.length)) B[i] = uniform(0, 1);
  auto watch = StopWatch(AutoStart.yes);
  sched.boot!dmm_dawn(0, N, 0, N, 0, N, 128);
  writeln(2.0 / watch.peek.total!"nsecs" * N * N * N, "GFLOPS");
}
```

以上で、行列積の並列処理が完成した。以下の操作でコンパイルする。最適化は有効に、実行時の境界検査は無効にする。

```bash
$ ldc -O -release -boundscheck=off -of=dmm dmm.d
$ for c in {0..35}; do taskset -c 0-$c ./dmm; done
```

Fig. 4.1は、2個のIntel Xeon E5-2699 v3を搭載した、NUMA型の共有メモリ環境で、台数効果を描画した結果である。
2軸での分割は、並列化の費用対効果を大幅に損ねた。再帰的な並列化とワークスティーリングの併用は、効果的である。

![scales/dawn.dmm.rank8192.gran128.pad32.ldc.xeon.e5.2699.v3.core36.png](/images/dawn.dmm.rank8192.gran128.pad32.ldc.xeon.e5.2699.v3.core36.png)

Fig. 4.1 dense matrix multiplication,  $8192\times8192\times8192$ .

同じ並列化でも、提案実装と既存実装の性能には差がある。その差を解釈する。まず、排他制御の所要時間を $D$ とする。
排他制御を $M/D/1$ の待ち行列とし、 $N$ 個のプロセッサが**ポアソン到着**すると、待ち時間 $w$ の期待値は式 4.1となる。

$$\begin{alignedat}{1}

w &= \left\lbrace N-1-\displaystyle\frac{1}{\rho b_{N-1}}\left(\displaystyle\sum_{n=0}^{N-1}b_n-N\right)\right\rbrace D,\\
b_n &= \displaystyle\sum_{k=0}^n\displaystyle\frac{(-1)^k}{k!}(n-k)^ke^{(n-k)\rho}\rho^k,\\
\rho &= \lambda D.
\end{alignedat} \qquad(4.1)$$

特に細粒度の並列処理の場合は、既存実装のままプロセッサを増強すると、到着率 $\lambda$ が増加して、待ち時間が急増する。
逆に、常に疎粒度のタスクを奪う提案実装では、到着率 $\lambda$ が抑制される。式 4.1は、Brun & Garcia (2000)に従った。

