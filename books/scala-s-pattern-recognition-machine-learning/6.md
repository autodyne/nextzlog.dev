---
title: 6 混合正規分布と最尤推定
---
## 6 混合正規分布と最尤推定

適当な観測量 $\boldsymbol{x}$ から、それが従う確率分布 $p$ を推定する手法が最尤推定である。具体的には、分布 $p$ の母数を推定する。

$$\forall\boldsymbol{x}\colon \boldsymbol{x} =
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_D
\end{pmatrix}
\sim p\left(\boldsymbol{x}\right). \qquad(6.1)$$

例えば、正規分布 $\mathcal{N}$ を仮定する場合は、平均 $\boldsymbol{\mu}$ と分散 $S$ が母数に該当する。ただし、分散 $S$ とは分散共分散行列を指す。

$$\mathcal{N}\left(\boldsymbol{x}\,\middle|\,\boldsymbol{\mu},S\right) =
\displaystyle\frac{1}{\tilde{\mathcal{N}}\left(S\right)} \exp \left\lbrace -\displaystyle\frac{1}{2} {}^t(\boldsymbol{x}-\boldsymbol{\mu}) S^{-1} (\boldsymbol{x}-\boldsymbol{\mu})\right\rbrace ,
\enspace\mathrm{where}\enspace
\tilde{\mathcal{N}}\left(S\right) = \sqrt{(2\pi)^D\left|S\right|}. \qquad(6.2)$$

正規分布では簡単なので、複数の正規分布の線型和を考えよう。式 6.3を**混合正規分布**と呼ぶ。定数 $w_k$ は加重である。

$$\boldsymbol{x} \sim p\left(\boldsymbol{x}\right) = \displaystyle\sum_{k=1}^K w_k \mathcal{N}\left(\boldsymbol{x}\,\middle|\,\boldsymbol{\mu}_k,S_k\right),
\enspace\mathrm{where}\enspace
\displaystyle\sum_{k=1}^K w_k = 1. \qquad(6.3)$$

正規分布を点 $\boldsymbol{x}$ の集団または**クラスタ**と見做せば、混合正規分布の最尤推定は、点 $\boldsymbol{x}$ が属す集団 $C_k$ の推定と同義である。

$$P\left(\boldsymbol{x} \in C_k\right) =
\displaystyle\frac{P\left(C_k\right)P\left(\boldsymbol{x}\,\middle|\,C_k\right)}{P\left(\boldsymbol{x}\right)} =
\displaystyle\frac{w_k \mathcal{N}\left(\boldsymbol{x}\,\middle|\,\boldsymbol{\mu}_k,S_k\right)}{p\left(\boldsymbol{x}\right)}. \qquad(6.4)$$

点 $\boldsymbol{x}$ がどの集団 $C_k$ に属すかは観測できず、潜在的な情報である。この情報を変数 $z$ で表すと、変数 $z$ は潜在変数となる。
Fig. 6.1は、混合正規分布の例である。式 6.3の母数を推定し、点 $\boldsymbol{x}$ で支配的な集団を求めれば、点 $\boldsymbol{x}$ の帰属がわかる。

![images/gmm.truth.png](/images/gmm.truth.png)

(1) cluster map.

![images/gmm.dense.png](/images/gmm.dense.png)

(2) density map.

Fig. 6.1 Ground-truth data of a Gaussian mixture model.

Fig. 6.1(1)の分類問題を、**クラスタリング**と呼ぶ。推定対象の値が潜在変数な点を指して、教師なし学習とも呼ばれる。

### 6.1 クラスタリングの実装

第6.1節では、混合正規分布や最尤推定の議論は忘れて、集合を最適なクラスタに分割する、素朴な方法を検討しよう。
理想的な集合 $C_k$ では、その要素 $\boldsymbol{x}$ と、集合 $C_k$ の重心 $\boldsymbol{\mu}_k$ の距離が最短となる。この命題を定式化して、式 6.5を得る。

$$\min \mathcal{D} =
\min \displaystyle\sum_{n=1}^N \displaystyle\sum_{k=1}^K z_{nk} \left\|\boldsymbol{x}_n-\boldsymbol{\mu}_k\right\|^2,
\enspace\mathrm{where}\enspace
\hat{z}_{nk} =
\begin{cases}
1,& \text{if \(\boldsymbol{x}_n     \in C_k\)},\\
0,& \text{if \(\boldsymbol{x}_n \not\in C_k\)}.
\end{cases} \qquad(6.5)$$

式 6.5の最適化は、逐次的に行う。まず、重心 $\boldsymbol{\mu}_k$ を乱数で初期化する。次に、式 6.6に従って、変数 $z_{nk}$ を修正する。

$$\hat{z}_{nk} =
\begin{cases}
1,& \text{if \(k=   \mathrm{arg\,min}_j\left\|\boldsymbol{x}_n-\boldsymbol{\mu}_j\right\|^2\)},\\
0,& \text{if \(k\neq\mathrm{arg\,min}_j\left\|\boldsymbol{x}_n-\boldsymbol{\mu}_j\right\|^2\)}.
\end{cases} \qquad(6.6)$$

最後に、式 6.7により、重心 $\boldsymbol{\mu}_k$ を修正する。式 6.7は、変数 $z_{nk}$ を固定して、式 6.5を重心 $\boldsymbol{\mu}_k$ で微分すると導ける。

$$\hat{\boldsymbol{\mu}}_k = \displaystyle\frac{1}{N_k} \displaystyle\sum_{n=1}^N z_{nk} \boldsymbol{x}_n,
\enspace\mathrm{where}\enspace
N_k = \displaystyle\sum_{n=1}^N z_{nk},
\Leftarrow
\displaystyle\frac{\partial \mathcal{D}}{\partial \boldsymbol{\mu}_k} = 0. \qquad(6.7)$$

以上の手順を繰り返し、最適解を得る。この手法を $k$ -*means*と呼ぶ。混合正規分布を仮定した考察は、第6.2節で行う。
以下に実装する。引数は、分割を行う点 $\boldsymbol{x}$ の集合と、分割後に得られる集団 $C_k$ の個数と、操作を繰り返す回数である。

```scala
class Kmeans(x: Seq[Seq[Double]], k: Int, epochs: Int = 100) {
  val mu = Array.fill(k, x.map(_.size).min)(math.random)
  def apply(x: Seq[Double]) = mu.map(quads(x)(_).sum).zipWithIndex.minBy(_._1)._2
  def quads(a: Seq[Double])(b: Seq[Double]) = a.zip(b).map(_-_).map(d=> d * d)
  def estep = x.groupBy(apply).values.map(c=> c.transpose.map(_.sum / c.size))
  for(epoch <- 1 to epochs) estep.zip(mu).foreach(_.toArray.copyToArray(_))
}
```

Fig. 6.2は、2個の正規分布の混合分布に従うFig. 6.1の散布図を $K$ 個の集団に分割した様子で、星型の点は重心を表す。

![images/gmm.km.k2.png](/images/gmm.km.k2.png)

(1)  $K=2$ .

![images/gmm.km.k3.png](/images/gmm.km.k3.png)

(2)  $K=3$ .

Fig. 6.2  $k$ -means clustering on Gaussian mixture model.

なお、正規分布の分散を考慮せず、同じ広がりを持つ集団を想定した点が、課題である。その様子はFig. 6.2にも窺える。

### 6.2 期待値最大化法の理論

第6.2節では、潜在変数 $z$ を、その値が確率的に決まる**確率変数**と考え、分散を含む、混合正規分布の母数を推定しよう。
観測変数 $\boldsymbol{x}$ に対し、潜在変数 $z$ の確率は、式 6.8で求まる。観測に基づき推定した確率なので、これを事後確率と呼ぶ。

$$P\left(z_{nk}\,\middle|\,\boldsymbol{x}_n,\theta\right) = \displaystyle\frac{w_k \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right)}{p\left(\boldsymbol{x}_n\right)} = \gamma_{nk}. \qquad(6.8)$$

次に、混合正規分布の尤度を定義する。尤度 $\mathcal{L}\left(\theta\right)$ は母数 $\theta$ の妥当性を表し、尤度の最大値を探す操作が最尤推定である。

$$\mathcal{L}\left(\theta\right) =
P\left(\boldsymbol{x}\,\middle|\,\theta\right) =
\displaystyle\prod_{n=1}^N \displaystyle\sum_{k=1}^K w_k \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right). \qquad(6.9)$$

微分計算の都合により、尤度を対数化して、対数尤度を最大化する母数を計算しよう。重心 $\boldsymbol{\mu}_k$ による偏微分の例を示す。

$$\displaystyle\frac{\partial }{\partial \boldsymbol{\mu}_k}\log\mathcal{L}\left(\theta\right) =
\displaystyle\frac{\partial }{\partial \boldsymbol{\mu}_k}\displaystyle\sum_{n=1}^N \log\displaystyle\sum_{k=1}^K w_k \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right) =
\displaystyle\sum_{n=1}^N \gamma_{nk} S_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k). \qquad(6.10)$$

加重と重心と分散の推定値 $\hat{w}_k,\hat{\boldsymbol{\mu}}_k,\hat{S}_k$ は式 6.11となる。加重のみ、式 6.3より、**ラグランジュの未定乗数法**で求めた。

$$\hat{w_k} = \displaystyle\frac{N_k}{N},\;
\left\lbrace 
\begin{aligned}
\hat{\boldsymbol{\mu}_k} &= \displaystyle\frac{1}{N_k} \displaystyle\sum_{n=1}^N \gamma_{nk} \boldsymbol{x}_n,\\
\hat{S_k} &= \displaystyle\frac{1}{N_k} \displaystyle\sum_{n=1}^N \gamma_{nk} (\boldsymbol{x}_n - \hat{\boldsymbol{\mu}}_k) {}^t(\boldsymbol{x}_n - \hat{\boldsymbol{\mu}}_k),
\end{aligned}
\right\rbrace 
\enspace\mathrm{where}\enspace
N_k = \displaystyle\sum_{n=1}^N \gamma_{nk}. \qquad(6.11)$$

式 6.11より、事後確率 $\gamma$ が求まれば、母数も求まるが、式 6.8より、事後確率 $\gamma$ の計算には、母数の値が必要である。
従って、解析的な求解は困難である。ここで、凸関数 $f$ と、正の実数 $\gamma_n$ は、式 6.12の**イェンゼンの不等式**を満たす。

$$\displaystyle\sum_{n=1}^N \gamma_n f(x_n) \geq f\left(\displaystyle\sum_{n=1}^N \gamma_n x_n\right),
\enspace\mathrm{where}\enspace
\displaystyle\sum_{n=1}^N \gamma_n=1. \qquad(6.12)$$

対数が凹関数である点に注意して、式 6.12に式 6.9を代入して、式 6.13の関数 $Q$ を得る。これを**補助関数**と呼ぶ。

$$\log \mathcal{L}\left(\theta\right) =
\max_\gamma Q(\gamma,\theta) \geq
\displaystyle\sum_{n=1}^N \displaystyle\sum_{k=1}^K \gamma_{nk} \log \displaystyle\frac{w_k \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right)}{\gamma_{nk}} =
Q(\gamma,\theta). \qquad(6.13)$$

補助関数 $Q$ は、式 6.14に示す、変数 $\gamma,\theta$ の修正を交互に繰り返すと単調増加し、最終的に、有限な実数値に収束する。

$$\left\lbrace 
\begin{aligned}
\hat{\gamma}^{t+1} &= \mathrm{arg\,max}_{\gamma} Q(\gamma,\theta^t), \\
\hat{\theta}^{t+1} &= \mathrm{arg\,max}_{\theta} Q(\gamma^t,\theta).
\end{aligned}
\right. \qquad(6.14)$$

式 6.14で、変数 $\gamma^t,\theta^t$ の最適値を求めると、式 6.8と式 6.11を得る。両者を交互に修正すると、尤度が最大化する。
式 6.8で変数 $\gamma$ を修正する操作は、式 6.15に示す、対数尤度の期待値を計算する操作である。これを*E-step*と呼ぶ。

$$\underset{z}{\mathbf{E}}\!\left[\,\log P\left(\boldsymbol{x},z\,\middle|\,\theta\right)\,\right] =
\int_z P\left(z\,\middle|\,\boldsymbol{x},\theta\right) \log P\left(\boldsymbol{x},z\,\middle|\,\theta\right) dz =
\displaystyle\sum_{n=1}^N \displaystyle\sum_{k=1}^K \gamma_{nk} \log \left\lbrace w_k \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right)\right\rbrace . \qquad(6.15)$$

式 6.11で変数 $\theta$ を修正する操作は、尤度を最大化する。これを*M-step*と呼び、両者を合わせて**期待値最大化法**と呼ぶ。
なお、単位行列 $E$ と実数値 $\lambda$ を使って、分散を $\lambda E$ と置くと、極限 $\lambda\to0$ で式 6.16が成立し、変数 $\gamma_{nk}$ も $z_{nk}$ になる。

$$\lim_{\lambda\to0} \lambda\log \left\lbrace w \mathcal{N}\left(\boldsymbol{x}\,\middle|\,\boldsymbol{\mu},\lambda E\right)\right\rbrace  =
\lim_{\lambda\to0} \left\lbrace \lambda\log w - \lambda\displaystyle\frac{D}{2} \log (2\pi\lambda) - \displaystyle\frac{1}{2} \left\|\boldsymbol{x}-\boldsymbol{\mu}\right\|^2\right\rbrace  =
-\displaystyle\frac{1}{2} \left\|\boldsymbol{x}-\boldsymbol{\mu}\right\|^2. \qquad(6.16)$$

即ち、式 6.17が成立し、その最大化は式 6.5の最小化に帰結する。 $k$ -*means*は、期待値最大化法の特殊な例と言える。

$$\lim_{\lambda\to0} \lambda \underset{\boldsymbol{z}}{\mathbf{E}}\!\left[\,\log P\left(\boldsymbol{x},z\,\middle|\,\theta\right)\,\right] =
-\displaystyle\frac{1}{2} \displaystyle\sum_{n=1}^N \displaystyle\sum_{k=1}^K z_{nk} \left\|\boldsymbol{x}_n-\boldsymbol{\mu}_k\right\|^2. \qquad(6.17)$$

また、期待値最大化法も、第6.4節で学ぶ変分ベイズ法の特殊な場合であり、第6.2節と酷似した式が、何度か登場する。

### 6.3 期待値最大化法の実装

第6.2節の議論に基づき、期待値最大化法を実装する。まず、 $K$ 個の $D$ 変量正規分布からなる混合正規分布を実装する。

```scala
class GMM(val d: Int, val k: Int) {
  val w = Array.fill(k)(1.0 / k)
  val m -> s = (Array.fill(k, d)(math.random), Array.fill(k, d)(math.random))
  def apply(x: Seq[Double]) = w.lazyZip(m).lazyZip(s).map(Normal(x)(_,_,_).p)
}
```

正規分布も実装する。引数は、加重と平均と分散である。なお、分散共分散行列を対角行列と仮定し、実装を単純化した。

```scala
case class Normal(x: Seq[Double])(w: Double, m: Seq[Double], s: Seq[Double]) {
  def n = math.exp(-0.5 * x.zip(m).map(_-_).map(d=>d*d).zip(s).map(_/_).sum)
  def p = w * n / math.pow(2 * math.Pi, 0.5 * x.size) / math.sqrt(s.product)
}
```

次に、最適化の手順を実装する。期待値最大化法の*E-step*と*M-step*を繰り返す。また、点 $\boldsymbol{x}$ が属す集団 $C_k$ を推定する。

```scala
class EM(val x: Seq[Seq[Double]], val mm: GMM, epochs: Int = 100) {
  def mstep(P: Seq[Seq[Double]]) = {
    P.map(_.sum / x.size).copyToArray(mm.w)
    val m = P.map(_.zip(x).map((p,x) => x.map(x => p * x)).transpose.map(_.sum))
    val s = P.map(_.zip(x).map((p,x) => x.map(x => p*x*x)).transpose.map(_.sum))
    m.zip(P).map((m,p) => m.map(_ / p.sum)).zip(mm.m).foreach(_.copyToArray(_))
    s.zip(P).map((s,p) => s.map(_ / p.sum)).zip(mm.s).foreach(_.copyToArray(_))
    for((s,m) <- mm.s.zip(mm.m); d <- 0 until mm.d) s(d) -= m(d) * m(d)
  }
  for(epoch <- 1 to epochs) mstep(x.map(mm(_)).map(p=>p.map(_/p.sum)).transpose)
}
```

Fig. 6.3は、Fig. 6.1と同じ散布図を、期待値最大化法で学習した結果で、Fig. 6.1と同様に、確率密度関数を可視化した。

![images/gmm.em.k2.png](/images/gmm.em.k2.png)

(1)  $K=2$ .

![images/gmm.em.k3.png](/images/gmm.em.k3.png)

(2)  $K=3$ .

Fig. 6.3 expectation maximization on a Gaussian mixture model.

期待値最大化法では、Fig. 6.2の $k$ -*means*と比較して、正規分布の密度の強弱を、境界付近の色分けに正しく反映できる。

### 6.4 変分ベイズ推定の理論

第6.2節の最尤推定では、母数の最適値を推定した。第6.4節で議論するベイズ推定では、母数の確率分布を推定できる。
特に、最適解が複数ある場合にも対応でき、過学習の抑制効果も期待できる。議論を始めるに当たり、尤度を定義しよう。

$$\mathcal{L}\left(\theta\right) =
p\left(\boldsymbol{x}\,\middle|\,\theta\right) =
\int p\left(\boldsymbol{x},z\right) dz =
\int p\left(\boldsymbol{x}\,\middle|\,z\right) p\left(z\,\middle|\,\theta\right) dz. \qquad(6.18)$$

第6.4節では、潜在変数 $z$ に加え、母数 $\theta$ も確率変数に含める。母数 $\theta$ の確率分布に母数 $\phi$ を設定し、尤度を定義し直す。

$$\mathcal{L}\left(\phi\right) =
p\left(\boldsymbol{x}\,\middle|\,\phi\right) =
\iint p\left(\boldsymbol{x},z,\theta\,\middle|\,\phi\right) dz d\theta =
\iint p\left(\boldsymbol{x}\,\middle|\,z\right) p\left(z\,\middle|\,\theta\right) p\left(\theta\,\middle|\,\phi\right) dz d\theta. \qquad(6.19)$$

式 6.18に対し、式 6.19を**周辺尤度**と呼ぶ。第6.2節と同様に、補助関数 $F$ を定義する。関数 $\hat{p}$ は、適当な分布である。

$$\log \mathcal{L}\left(\phi\right) =
\log \iint \hat{p}\left(z,\theta\right) \displaystyle\frac{p\left(\boldsymbol{x},z,\theta\right)}{\hat{p}\left(z,\theta\right)} dz d\theta \geq
\iint \hat{p}\left(z,\theta\right) \log \displaystyle\frac{p\left(\boldsymbol{x},z,\theta\right)}{\hat{p}\left(z,\theta\right)} dz d\theta = F(\hat{p}). \qquad(6.20)$$

補助関数 $F$ を最大化すると、尤度 $\mathcal{L}$ に収束する。その差は、式 6.21に示す**カルバック・ライブラー情報量**の形になる。
式 6.21は、変数 $z,\theta$ が従う分布 $\hat{p}$ を仮定した場合の、分布 $\hat{p},p$ の平均情報量の差である。両者が同じ場合に $0$ となる。

$$\log \mathcal{L}\left(\boldsymbol{x}\right) - F(\hat{p}) =
\iint \hat{p}\left(z,\theta\right) \log p\left(\boldsymbol{x}\right) dz d\theta - F(\hat{p}) =
\iint \hat{p}\left(z,\theta\right) \log \displaystyle\frac{\hat{p}\left(z,\theta\right)}{p\left(z,\theta\,\middle|\,\boldsymbol{x}\right)} dz d\theta =
D\!\left(\hat{p}\|p\right) \geq 0. \qquad(6.21)$$

関数 $\hat{p}$ を引数に取る関数 $F$ を、**汎関数**と呼ぶ。汎関数 $F$ の極値を与える引数 $\hat{p}$ を探索する問題は、**変分問題**と呼ばれる。
残念ながら、複数の引数を取る関数 $\hat{p}$ の探索は難しく、式 6.22に示す**平均場近似**により、変数間の独立性を仮定する。

$$\hat{p}\left(z,\theta\right) = f(z)g(\theta),
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\int f(z) dz &= 1,\\
\int g(\theta) d\theta &= 1.
\end{aligned}
\right. \qquad(6.22)$$

関数 $f,g$ に対する汎関数 $F$ の変分問題を解く。ここで、式 6.23に示す**オイラー・ラグランジュ方程式**の特殊形を使う。

$$\displaystyle\frac{\partial }{\partial f} \displaystyle\frac{\partial F}{\partial z} =
\displaystyle\frac{\partial }{\partial f} \int f(z) g(\theta) \log \displaystyle\frac{p\left(\boldsymbol{x},z,\theta\right)}{f(z)g(\theta)} d\theta = 0. \qquad(6.23)$$

関数 $f$ の値を固定し、単に変数と考えて偏微分すると、式 6.24を得る。関数 $g$ に対し、式 6.22の制約条件を使った。

$$\displaystyle\frac{\partial }{\partial f} \int f(z) g(\theta) \log \displaystyle\frac{p\left(\boldsymbol{x},z,\theta\right)}{f(z)g(\theta)} d\theta =
\int g(\theta) \log \displaystyle\frac{p\left(\boldsymbol{x},z,\theta\right)}{f(z)g(\theta)} d\theta - 1 = 0. \qquad(6.24)$$

式 6.24から、関数 $f$ の最適値を求める。式 6.22の近似で仮定した、変数 $z,\theta$ 間の独立性より、式 6.25が成立する。

$$\log f(z) =
\log f(z) \int g(\theta) d\theta =
\int g(\theta) \log f(z) d\theta. \qquad(6.25)$$

関数 $f,g$ の最適値 $\hat{f},\hat{g}$ は、式 6.26となる。関数 $f,g$ を交互に修正すると、補助関数 $F$ が増加し、周辺尤度に収束する。
式 6.26は、式 6.14の*E-step*と*M-step*に対応し、第6.2節で学んだ期待値最大化法に対し、**変分ベイズ法**と呼ばれる。

$$\left\lbrace 
\begin{alignedat}{2}
\hat{f}(z) &\propto \exp \int g(\theta) \log p\left(\boldsymbol{x},z,\theta\right) d\theta &&= \exp \underset{g}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z,\theta\right)\,\right],\\
\hat{g}(\theta) &\propto \exp \int f(z) \log p\left(\boldsymbol{x},z,\theta\right) dz &&= \exp \underset{f}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z,\theta\right)\,\right].
\end{alignedat}
\right. \qquad(6.26)$$

なお、母数 $\theta$ に対し、適当な事前分布を設定すると、分布 $\hat{g}$ と事前分布 $p$ の乖離を抑制し、過学習を防ぐ効果が生じる。

$$F(\hat{p}) =
\iint f(z)g(\theta) \log \displaystyle\frac{p\left(\boldsymbol{x},z\,\middle|\,\theta\right)}{f(z)} \displaystyle\frac{p\left(\theta\right)}{g(\theta)} dz d\theta =
\underset{f,g}{\mathbf{E}}\!\left[\,\log \displaystyle\frac{p\left(\boldsymbol{x},z\,\middle|\,\theta\right)}{f(z)}\,\right] - D\!\left(g(\theta)\|p\left(\theta\right)\right). \qquad(6.27)$$

無限の個数の点 $\boldsymbol{x}_n$ を学習した場合の尤度は、**ラプラス近似**で式 6.28と近似でき、**ベイズ情報量基準**の形が出現する。

$$F(\hat{p}) \simeq
\underset{f,g}{\mathbf{E}}\!\left[\,\log \displaystyle\frac{p\left(\boldsymbol{x},z\,\middle|\,\theta\right)}{f(z)}\,\right] - \displaystyle\frac{\hat{\left|\theta\right|}}{2} \log N + \log p\left(\hat{\theta}\right). \qquad(6.28)$$

式 6.28には、疎な基底を学習し、母数の個数 $\left \vert \theta\right \vert$ を実質的に削減する**正則化**の効果があり、過学習の抑制が期待できる。

### 6.5 母数の事前分布の設定

潜在変数 $z$ や母数 $\theta$ の事前分布を注意深く設定すると、事前分布と事後分布が同じ形の分布になり、計算が容易になる。
これを**共役分布**と呼ぶ。混合正規分布の母数にも、共役分布が存在する。まず、潜在変数 $z$ が多項分布に従うと仮定する。

$$p\left(z\,\middle|\,w\right) = \displaystyle\prod_{n=1}^N \displaystyle\prod_{k=1}^K w_k^{z_{nk}},
\enspace\mathrm{where}\enspace
\forall n\colon
\displaystyle\sum_{k=1}^K z_{nk} = 1. \qquad(6.29)$$

式 6.29は、潜在変数 $z$ に対する加重 $w$ の尤度でもある。加重 $w$ の事前分布を、式 6.30のディリクレ分布で定義する。
これは、 $K$ 個の排反事象の反復試行で、事象 $k$ の出現が $\alpha_k-1$ 回だった場合に、事象 $k$ の確率が $w_k$ である確率を表す。

$$p\left(w\right) = \mathrm{Dir}\left(w\,\middle|\,\alpha\right) =
\Gamma\left(\displaystyle\sum_{k=1}^K \alpha_k\right) \displaystyle\prod_{k=1}^K \displaystyle\frac{w_k^{\alpha_k-1}}{\Gamma\left(\alpha_k\right)} =
\displaystyle\frac{1}{\mathrm{B}\left(\alpha\right)} \displaystyle\prod_{k=1}^K w_k^{\alpha_k-1}. \qquad(6.30)$$

関数 $\Gamma$ はガンマ関数で、階乗を拡張した複素関数である。関数 $\mathrm{B}$ はベータ関数で、多項係数を拡張した複素関数である。
平均 $\boldsymbol{\mu}$ の事前分布には、式 6.31の正規分布を仮定する。母数 $\sigma$ には、式 6.31の分散を徐々に減少させる効果がある。

$$p\left(\boldsymbol{\mu}\,\middle|\,S\right) = \displaystyle\prod_{k=1}^K \mathcal{N}\left(\boldsymbol{\mu}_k\,\middle|\,\boldsymbol{m}_k,\sigma_k^{-1} S_k\right). \qquad(6.31)$$

式 6.31は、平均 $\boldsymbol{\mu}$ に対する分散 $S$ の尤度でもある。分散 $S$ の事前分布は、式 6.32の**逆ウィシャート分布**を仮定する。

$$p\left(S\right) =
\displaystyle\prod_{k=1}^K \mathcal{W}\left(S_k^{-1}\,\middle|\,W_k,\nu_k\right) =
\displaystyle\prod_{k=1}^K \displaystyle\frac{1}{\tilde{\mathcal{W}}\left(W_k,\nu_k\right)} \left|S_k^{-1}\right|^{\frac{\nu_k-D-1}{2}} \exp\left\lbrace -\displaystyle\frac{1}{2}\mathrm{tr}\left(W_k^{-1}S_k^{-1}\right)\right\rbrace . \qquad(6.32)$$

これは、分散 $W$ の $D$ 変量正規分布に従う $\nu$ 個の変数 $\boldsymbol{x}_n$ の直積 $\boldsymbol{x}_n{}^t\boldsymbol{x}_n$ の和の分布である。即ち、標本分散の分布である。

$$\tilde{\mathcal{W}}\left(W_k,\nu_k\right) = 2^{\frac{\nu_kD}{2}} \pi^{\frac{D(D-1)}{4}} \left|W_k\right|^{\frac{\nu_k}{2}} \displaystyle\prod_{d=0}^{D-1} \Gamma\left(\displaystyle\frac{\nu_k-d}{2}\right). \qquad(6.33)$$

Fig. 6.4は、Fig. 6.1と同じ散布図を、変分ベイズ法で学習した結果で、Fig. 6.1と同様に、確率密度関数を可視化した。

![images/gmm.vb.k2.png](/images/gmm.vb.k2.png)

(1)  $K=2$ .

![images/gmm.vb.k3.png](/images/gmm.vb.k3.png)

(2)  $K=3$ .

Fig. 6.4 variational Bayesian inference on a Gaussian mixture model.

正則化の恩恵により、集団の個数を過剰に設定した場合でも、余剰の集団の加重が徐々に低下し、過学習が抑制される。

### 6.6 母数の事後分布の導出

変分ベイズ法の式 6.26に対し、第6.5節で設定した共役事前分布を代入する。まず、全ての変数の結合確率を求める。

$$p\left(\boldsymbol{x},z,w,\boldsymbol{\mu},S\right) =
p\left(\boldsymbol{x},z\,\middle|\,w,\boldsymbol{\mu},S\right) p\left(w,\boldsymbol{\mu},S\right) =
p\left(\boldsymbol{x}\,\middle|\,z,\boldsymbol{\mu},S\right) p\left(z\,\middle|\,w\right) p\left(w) p\left(\boldsymbol{\mu}\,\middle|\,S\right) p(S\right). \qquad(6.34)$$

*E-step*を導く。母数 $\theta$ の事前分布を固定し、潜在変数 $z$ の分布を最適化する操作なので、その間に式 6.35が成立する。

$$f(z) \propto
\exp \underset{g}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z\,\middle|\,w,\boldsymbol{\mu},S\right)\,\right] =
\exp \displaystyle\sum_{n=1}^N \displaystyle\sum_{k=1}^K z_{nk} \left\lbrace \underset{w}{\mathbf{E}}\!\left[\,\log w_k\,\right] + \underset{\boldsymbol{\mu},S}{\mathbf{E}}\!\left[\,\log \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right)\,\right]\right\rbrace  \propto
\displaystyle\prod_{n=1}^N \displaystyle\prod_{k=1}^K \gamma_{nk}^{z_{nk}}. \qquad(6.35)$$

式 6.35に現れる、加重 $w$ の対数の期待値は、式 6.36となる。関数 $\psi$ は**ディガンマ関数**で、関数 $\Gamma$ の対数微分である。

$$\underset{w}{\mathbf{E}}\!\left[\,\log w_k\,\right] =
\displaystyle\frac{1}{\mathrm{B}\left(\alpha\right)} \displaystyle\frac{\partial }{\partial \alpha_k} \int \displaystyle\prod_{j=1}^K w_j^{\alpha_j-1} dw =
\displaystyle\frac{\partial }{\partial \alpha_k} \log \mathrm{B}\left(\alpha\right) =
\psi\left(\alpha_k\right) - \psi\left(\displaystyle\sum_{j=1}^K \alpha_j\right). \qquad(6.36)$$

式 6.35に現れる、正規分布の対数の期待値は、式 6.2の正規分布の確率密度関数より、式 6.37の形に分解できる。

$$\underset{\boldsymbol{\mu},S}{\mathbf{E}}\!\left[\,\log \mathcal{N}\left(\boldsymbol{x}_n\,\middle|\,\boldsymbol{\mu}_k,S_k\right)\,\right] =
- \displaystyle\frac{1}{2} \underset{\boldsymbol{\mu},S}{\mathbf{E}}\!\left[\,D \log 2\pi + \log \left|S_k\right| + {}^t(\boldsymbol{x}_n - \boldsymbol{\mu}_k) S_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)\,\right]. \qquad(6.37)$$

式 6.37に現れる、行列式 $\left \vert S\right \vert$ の対数の期待値は、式 6.32の確率密度関数を母数 $\nu_k$ で偏微分すれば、式 6.38となる。

$$\underset{S}{\mathbf{E}}\!\left[\,\log \left|S_k\right|\,\right] =
2 \int \displaystyle\frac{\partial \tilde{\mathcal{W}}}{\partial \nu_k}\displaystyle\frac{\mathcal{W}}{\tilde{\mathcal{W}}} dS - 2 \int \displaystyle\frac{\partial \mathcal{W}}{\partial \nu_k} dS =
\displaystyle\frac{2}{\tilde{\mathcal{W}}} \displaystyle\frac{\partial \tilde{\mathcal{W}}}{\partial \nu_k} =
- D\log 2 - \log\left|W_k\right| - \displaystyle\sum_{d=0}^{D-1} \psi\left(\displaystyle\frac{\nu_k-d}{2}\right). \qquad(6.38)$$

式 6.37に現れる、行列積の期待値は、母数 $\boldsymbol{\mu}$ が、式 6.31の正規分布に従う事実と因数分解により、式 6.39となる。

$$\underset{\boldsymbol{\mu},S}{\mathbf{E}}\!\left[\,{}^t(\boldsymbol{x}_n - \boldsymbol{\mu}_k) S_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)\,\right] =
\nu_k {}^t(\boldsymbol{x}_n - \boldsymbol{m}_k) W_k (\boldsymbol{x}_n - \boldsymbol{m}_k) + \displaystyle\frac{D}{\sigma_k}. \qquad(6.39)$$

*M-step*を導く。事後確率 $\gamma$ を式 6.40に代入し、事前分布と事後分布の共役性に注意して、母数の事後分布を求めよう。

$$g(\theta) \propto \exp\left\lbrace \underset{z}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z\,\middle|\,w,\boldsymbol{\mu},S\right)\,\right] + \log p\left(w) + \log p\left(\boldsymbol{\mu}\,\middle|\,S\right) + \log p(S\right)\right\rbrace . \qquad(6.40)$$

式 6.40で、変数 $\boldsymbol{x},z$ の結合確率の対数の期待値は、式 6.2の正規分布と式 6.29の多項分布より、式 6.41となる。

$$\underset{z}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z\,\middle|\,w,\boldsymbol{\mu},S\right)\,\right] =
- \displaystyle\frac{1}{2} \displaystyle\sum_{k=1}^K \displaystyle\sum_{n=1}^N \gamma_{nk}
\left\lbrace D \log 2\pi + \left|S_k\right| + {}^t(\boldsymbol{x}_n-\hat{\boldsymbol{\mu}}_k) S_k^{-1} (\boldsymbol{x}_n-\hat{\boldsymbol{\mu}}_k) - 2 \log w_k\right\rbrace . \qquad(6.41)$$

共役性より、母数 $\theta$ の事後分布は、事前分布の母数 $\phi$ を、推定値 $\hat{\phi}$ に置換した場合と等値であり、式 6.42が成立する。

$$\underset{z}{\mathbf{E}}\!\left[\,\log p\left(\theta\,\middle|\,\boldsymbol{x},z\right)\,\right] =
\log p\left(\theta\,\middle|\,\hat{\phi}\right) =
\underset{z}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z\,\middle|\,\theta\right)\,\right] + \log p\left(\theta\,\middle|\,\phi\right) - \underset{z}{\mathbf{E}}\!\left[\,\log p\left(\boldsymbol{x},z\right)\,\right]. \qquad(6.42)$$

式 6.42より、母数 $\alpha,\sigma,\nu$ の推定値に対して、式 6.43が成立する。変数 $N_k$ は、集団 $C_k$ の要素の個数の期待値を表す。

$$\hat{\alpha}_k - \alpha_k = \hat{\sigma}_k - \sigma_k = \hat{\nu}_k - \nu_k = N_k = \displaystyle\sum_{n=1}^N z_{nk}. \qquad(6.43)$$

母数 $\boldsymbol{m}$ の場合は、式 6.44が成立する。期待値最大化法の式 6.11を考えれば、事前分布と標本平均の加重平均である。

$$\hat{\boldsymbol{m}}_k = \displaystyle\frac{1}{\hat{\sigma}_k} \left(\sigma_k \boldsymbol{m}_k + \displaystyle\sum_{n=1}^N \gamma_{nk} \boldsymbol{x}_n\right). \qquad(6.44)$$

母数 $W$ の場合は、式 6.45が成立する。これも、式 6.45の右辺に着目すれば、事前分布と標本分散の加重平均である。

$$\hat{W}_k^{-1} =
W_k^{-1}+\displaystyle\sum_{n=1}^N\gamma_{nk}\boldsymbol{x}_n{}^t\boldsymbol{x}_n+\sigma_k\boldsymbol{m}_k{}^t\boldsymbol{m}_k-\hat{\sigma}_k\hat{\boldsymbol{m}}_k{}^t\hat{\boldsymbol{m}}_k. \qquad(6.45)$$

式 6.45の導出では、分散 $W$ の**コレスキー分解**により下三角行列 $T$ が存在して、式 6.46が成立する性質を利用した。

$${}^t\boldsymbol{x} W \boldsymbol{x} =
({}^t\boldsymbol{x} T) ({}^tT \boldsymbol{x}) =
{}^t({}^tT \boldsymbol{x}) ({}^tT \boldsymbol{x}) =
\mathrm{tr}\left(({}^tT \boldsymbol{x}) {}^t({}^tT \boldsymbol{x})\right) =
\mathrm{tr}\left({}^tT (\boldsymbol{x} {}^t\boldsymbol{x}) T\right) =
\mathrm{tr}\left((\boldsymbol{x} {}^t\boldsymbol{x}) W\right). \qquad(6.46)$$

事前分布と最尤推定の最適値の加重平均が現れる点には、式 6.27で議論した、事前分布による正則化の効果が窺える。

### 6.7 変分ベイズ推定の実装

第6.5節の議論に基づき、変分ベイズ推定を実装する。まず、本体を実装する。引数は、期待値最大化法と同等である。
式 6.43より、母数 $\alpha,\sigma,\nu$ は、初期値を揃えると、以後の最適化を通じて常に同じ値になる。そこで、同じ変数にした。

```scala
class VB(val x: Seq[Seq[Double]], val mm: GMM, epochs: Int = 1000, W: Double = 1) {
  val n = Array.fill(mm.k)(1.0 / mm.k)
  val w -> m = (Array.fill(mm.k, mm.d)(W), Array.fill(mm.k, mm.d)(math.random))
  for(epoch <- 1 to epochs) new MstepGMM(this, mm, new EstepGMM(this, mm).post)
}
```

*E-step*を実装する。式 6.35に基づき、潜在変数 $z$ の事後確率 $\gamma$ を計算する。最後に、総和で事後確率 $\gamma$ を規格化する。

```scala
class EstepGMM(vb: VB, mm: GMM) {
  val eq35 = vb.n.map(Digamma).map(_-Digamma(vb.n.sum))
  val eq3A = vb.n.map(n=>0.to(mm.d-1).map(d=>(n-d)/2).map(Digamma))
  val eq36 = eq3A.zip(vb.w).map(_.sum-_.map(math.log).sum).map(_/2)
  def wish = vb.x.toArray.map(_.toArray).map(vb.m-_).map(d=>d.mul(d).div(vb.w))
  def eq34 = wish.map(_.zip(vb.n).map(-_.sum/2*_))+eq35+eq36-vb.n.map(mm.d/_/2)
  def post = eq34.map(_.map(math.exp)).map(x=>x.map(_/x.sum)).toSeq.transpose
}
```

*M-step*を実装する。期待値最大化法の実装を流用して、母数の最尤推定値を計算し、事前分布との加重平均を計算する。

```scala
class MstepGMM(vb: VB, mm: GMM, post: Seq[Seq[Double]]) {
  new EM(vb.x, mm, 0).mstep(post)
  val eq11 = post.map(_.sum).toArray
  val eq38 = vb.n.zip(eq11).map(_+_)
  val eq39 = vb.m.mul(vb.n).div(eq38).add(mm.m.mul(eq11).div(eq38))
  val eq41 = vb.m.mul(vb.m).mul(vb.n).sub(eq39.mul(eq39).mul(eq38))
  val eq40 = mm.s.add(mm.m.mul(mm.m)).mul(eq11).add(vb.w.add(eq41))
  eq38.copyToArray(vb.n)
  eq39.zip(vb.m).foreach(_.copyToArray(_))
  eq40.zip(vb.w).foreach(_.copyToArray(_))
}
```

*M-step*では、平均や分散の配列の四則演算が頻繁に現れる。簡潔な実装を目指し、暗黙の型変換で四則演算を実現した。

```scala
implicit class Vector(x: Array[Array[Double]]) {
  def +(y: Array[Double]) = x.map(_.zip(y).map(_+_))
  def -(y: Array[Double]) = x.map(_.zip(y).map(_-_))
  def add(y: Array[Double]) = x.zip(y).map((x,y) => x.map(_+y))
  def sub(y: Array[Double]) = x.zip(y).map((x,y) => x.map(_-y))
  def mul(y: Array[Double]) = x.zip(y).map((x,y) => x.map(_*y))
  def div(y: Array[Double]) = x.zip(y).map((x,y) => x.map(_/y))
  def add(y: Array[Array[Double]]) = x.zip(y).map(_.zip(_).map(_+_))
  def sub(y: Array[Array[Double]]) = x.zip(y).map(_.zip(_).map(_-_))
  def mul(y: Array[Array[Double]]) = x.zip(y).map(_.zip(_).map(_*_))
  def div(y: Array[Array[Double]]) = x.zip(y).map(_.zip(_).map(_/_))
}
```

最後に、ディガンマ関数を実装する。詳細は省くが、**ワイエルシュトラスの無限乗積表示**を利用して、簡単に計算できる。

```scala
object Digamma extends Function[Double, Double] {
  def apply(x: Double): Double = {
    var index -> value = (x, 0.0)
    def d = 1.0 / (index * index)
    while(index < 49) (value -= 1 / index, index += 1)
    val s = d * (1.0 / 12 - d * (1.0 / 120 - d / 252))
    (value + math.log(index) - 0.5 / index - s)
  }
}
```
