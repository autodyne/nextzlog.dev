---
title: 4 決定木の学習と汎化性能
---
## 4 決定木の学習と汎化性能

意思決定の分野では、しばしば**決定木**と呼ばれる、質問と条件分岐の再帰的な木構造で、条件 $\boldsymbol{x}$ と結論 $y$ の関係を表す。
例えば、式 4.1は、海水浴の是非 $y$ を判断する決定木である。気象 $\boldsymbol{x}$ に対し、質問と条件分岐を繰り返し、結論を導く。

$$y \approx f(\boldsymbol{x}) =
\begin{cases}
0 & \text{if \(\mathrm{wavy}(\boldsymbol{x}) = 1\)} \\
\text{otherwise}
\left\lbrace 
\begin{aligned}
& 0 && \text{if \(\mathrm{rain}(\boldsymbol{x}) = 1\)} \\
& 1 && \text{if \(\mathrm{rain}(\boldsymbol{x}) = 0\)} \\
\end{aligned}
\right\rbrace 
& \text{if \(\mathrm{wavy}(\boldsymbol{x}) = 0\)} \\
\end{cases} \qquad(4.1)$$

決定木の学習では、意思決定の事例の集合 $\left\lbrace \boldsymbol{x},y\right\rbrace$  に対し、簡潔で解釈の容易な質問と条件分岐と、その順序を習得する。

### 4.1 情報源符号化定理

理想的な決定木は、簡潔明瞭である。即ち、最低限の質問で、結論に至る。ここで、**情報源符号化**の概念を導入しよう。
質問と条件分岐を繰り返す過程は、条件 $\boldsymbol{x}$ の情報を分解し、情報の断片に2進数の**符号語**を割り振る操作と等価である。

$$C(\boldsymbol{x}) \colon \boldsymbol{x} \mapsto \boldsymbol{s} \in \left\lbrace 0, 10, 11\right\rbrace . \qquad(4.2)$$

事象 $\boldsymbol{x}$ が従う確率分布 $p\left(\boldsymbol{x}\right)$ を仮定して、事象 $\boldsymbol{x}$ が伴う情報の価値 $I(\boldsymbol{x})$ を定義する。質問の妥当性に相当する量である。
情報の価値とは、その事象の希少性である。即ち、価値 $I(\boldsymbol{x})$ は、確率 $p\left(\boldsymbol{x}\right)$ に対して単調減少であり、式 4.3を満たす。

$$p\left(\boldsymbol{x}_i\right) \leq p\left(\boldsymbol{x}_j\right) \Leftrightarrow I(\boldsymbol{x}_i) \geq I(\boldsymbol{x}_j). \qquad(4.3)$$

また、複数の事象が同時に発生した場合の情報の価値は、個別に発生した場合の情報の価値の総和となると自然である。

$$I(\boldsymbol{x}_1,\boldsymbol{x}_2,\ldots,\boldsymbol{x}_N) = \displaystyle\sum_{n=1}^N I(\boldsymbol{x}_n). \qquad(4.4)$$

式 4.4の性質を**情報の加法性**と呼ぶ。以上の性質を満たす定義を考えると、式 4.5を得る。この $I(\boldsymbol{x})$ を**情報量**と呼ぶ。

$$I(\boldsymbol{x}) = \log_2 \displaystyle\frac{1}{p\left(\boldsymbol{x}\right)} = - \log_2 p\left(\boldsymbol{y}\right) \geq 0. \qquad(4.5)$$

符号 $C$ の符号語を圧縮するには、事象 $\boldsymbol{x}$ に、情報量に応じた長さの符号語を割り振る。これを**エントロピー符号**と呼ぶ。
符号語の長さの期待値 $\overline{L}$ は、式 4.6の**シャノンの情報源符号化定理**に従う。関数 $H$ を確率分布 $p$ の**平均情報量**と呼ぶ。

$$\overline{L(C)} \geq H(p) = \displaystyle\sum_{\boldsymbol{x}} p\left(\boldsymbol{x}\right) I(\boldsymbol{x}) = -\displaystyle\sum_{\boldsymbol{x}} p\left(\boldsymbol{x}\right) \log_2 p\left(\boldsymbol{x}\right) \geq 0. \qquad(4.6)$$

情報量の議論を利用して、質問の回数を圧縮する方法を検討しよう。まず、最初の質問は、情報量が最大の質問を選ぶ。
質問 $Q$ により、集合 $X$ が $K$ 通りの部分集合 $X_k$ に分割される場合は、質問 $Q$ の情報量 $G(Q)$ は、式 4.7で定義される。

$$G(Q) = H(X) - H(X|Q) = H(X) - \displaystyle\sum_{k=0}^{K-1} P\left(X_k\,\middle|\,X\right) H(X_k) \geq 0. \qquad(4.7)$$

同様に、部分集合 $X_k$ に対し、情報量が最大の質問を選び、次の質問とする。この操作を繰り返し、最適な決定木を得る。
質問の回数は整数なので、決定木が表す分布 $\hat{p}$ は、分布 $p$ と異なる。その様子を表す $H(p,\hat{p})$ を**交差エントロピー**と呼ぶ。

$$\overline{L(C)} = H(p,q) =
-\displaystyle\sum_{\boldsymbol{x}} p\left(\boldsymbol{x}\right) \log \hat{p}\left(\boldsymbol{x}\right) =
-\displaystyle\sum_{\boldsymbol{x}} p\left(\boldsymbol{x}\right) \left\lbrace \log p\left(\boldsymbol{x}\right) - \log\displaystyle\frac{p\left(\boldsymbol{x}\right)}{\hat{p}\left(\boldsymbol{x}\right)}\right\rbrace  =
H(p) + D\!\left(p\|q\right) \geq H(p). \qquad(4.8)$$

余分な質問の回数を表す $D\!\left(p\ \vert \hat{p}\right)$ を**カルバック・ライブラー情報量**と呼ぶ。これは、確率分布 $p,\hat{p}$ の差を表す量でもある。

### 4.2 条件分岐の最適化

第4.1節の議論に基づき、決定木を実装する。まず、推論を抽象化する。条件 $\boldsymbol{x}$ は整数の列で、結論 $y$ は任意の型とする。

```scala
trait Node[T] extends (Seq[Int] => T)
```

次に、決定木の本体を実装する。引数は、決定木が学習する集合 $\left\lbrace \boldsymbol{x},y\right\rbrace$  と、決定木の末端の細分化を抑える閾値である。
決定木は再帰的に生成されるが、質問の情報量が微小の場合は、分布 $p\left(y\right)$ の最大値を与える $y$ を定数値として出力する。

```scala
case class Question[T](x: Seq[(Seq[Int], T)], limit: Double = 1e-5) extends Node[T] {
  lazy val m = x.groupBy(_._2).maxBy(_._2.size)._1
  lazy val p = x.groupBy(_._2).map(_._2.size.toDouble / x.size)
  lazy val ent = p.map(p => -p * math.log(p)).sum / math.log(2)
  lazy val division = x.head._1.indices.map(split).minBy(_.ent)
  def apply(x: Seq[Int]) = if(ent - division.ent < limit) m else division(x)
  def split(v: Int) = x.map(_._1(v)).toSet.map(Division(x,v,_)).minBy(_.ent)
}
```

次に、条件分岐を実装する。引数は、決定木が学習する集合 $\left\lbrace \boldsymbol{x},y\right\rbrace$  と、条件 $\boldsymbol{x}$ を分割する軸と、分割を行う閾値である。
分割する軸と値は、式 4.7で議論した通り、質問の情報量を最大化する軸と値が選択される。これで決定木が完成した。

```scala
case class Division[T](x: Seq[(Seq[Int], T)], axis: Int, value: Int) extends Node[T] {
  val sn1 = Question(x.filter(_._1(axis) >  value))
  val sn2 = Question(x.filter(_._1(axis) <= value))
  val ent = (sn1.ent * sn1.x.size + sn2.ent * sn2.x.size) / x.size
  def apply(x: Seq[Int]) = if(x(axis) >= value) sn1(x) else sn2(x)
}
```

Fig. 4.1は、各々が正規分布に従う3クラスの点の集合を学習し、決定木で空間をそれらのクラスに分割した結果である。
結果的に、過剰に複雑な境界となった。個別の事例には忠実だが、正規分布の形からは乖離した。これを**過学習**と呼ぶ。

![images/id3.plain.png](/images/id3.plain.png)

(1)  $\mathtt{limit}=1e-5$ .

![images/id3.prune.png](/images/id3.prune.png)

(2)  $\mathtt{limit}=1e-1$ .

Fig. 4.1 region segmentation by a decision tree.

過学習は、機械学習で普遍的な課題だが、特に、決定木は、際限なく細分化できるため、しばしば表現能力が過剰である。
過学習を抑えるには、Fig. 4.1(2)のように、分割の閾値を調節するか、第4.3節で学ぶ**アンサンブル学習**が有効である。

### 4.3 アンサンブル学習

決定木に限らず、機械学習では、真の関係 $f$ と、習得した関係 $\hat{f}$ の間に若干の差があり、それが過学習として顕在化する。
過学習の原因は、教師データの偏りや、関数 $\hat{f}$ の過剰な表現能力にある。関数 $f,\hat{f}$ の差を2乗誤差関数で定式化しよう。

$$\int_{\boldsymbol{x}}p\left(\boldsymbol{x}\right)(y-\hat{f}(\boldsymbol{x}))^2d\boldsymbol{x} = \mathbf{V}\!\left[\,y-f(\boldsymbol{x})\,\right] + \left(\underset{}{\mathbf{E}}\!\left[\,\hat{f}(\boldsymbol{x})\,\right] - f(\boldsymbol{x})\right)^2 + \mathbf{V}\!\left[\,\hat{f}(\boldsymbol{x})\,\right]. \qquad(4.9)$$

式 4.9の第2項の平方根を**バイアス**と、第3項を**バリアンス**と呼ぶ。この2項が過学習や、逆に学習不足の原因となる。
両者はトレードオフの関係にあるが、 $T$ 個の関数 $\hat{f}_t$ を**弱学習器**とし、投票を行う**アンサンブル学習**により、調節できる。

$$\hat{f}(\boldsymbol{x}) = \displaystyle\frac{1}{T} \displaystyle\sum_{t=1}^T \hat{f}_t(\boldsymbol{x}). \qquad(4.10)$$

過学習を防ぐには、相互に独立した弱学習器の訓練が重要である。そこで、事例を選び直す**ブートストラップ法**を行う。
要素の重複を許して $T$ 通りの部分集合を作成し、 $T$ 通りの弱学習器 $f_t$ を訓練する。これで、式 4.11の分散が低減する。

$$\mathbf{V}\!\left[\,\hat{f}(\boldsymbol{x})\,\right] = \displaystyle\frac{1}{T^2}\displaystyle\sum_{i=1}^T \displaystyle\sum_{j=1}^T \mathbf{C}\!\left[\,f_i(\boldsymbol{x}),f_j(\boldsymbol{x})\,\right]. \qquad(4.11)$$

この手法は**バギング**とも呼ばれる。基本的には、決定木や深層学習など表現能力が過剰な手法に使う。以下に実装する。

```scala
case class Bagging[T](x: Seq[(Seq[Int], T)], t: Int, n: Int) extends Node[T] {
  val f = Seq.fill(t)(Question(Seq.fill(n)(x(util.Random.nextInt(x.size)))))
  def apply(x: Seq[Int]) = f.map(_(x)).groupBy(identity).maxBy(_._2.size)._1
}
```

### 4.4 ブースティング法

逆に、学習不足の解消には、弱学習器の表現能力を補う弱学習器を作り、加重投票を行う**ブースティング法**を適用する。

$$\hat{f}(\boldsymbol{x}) = \displaystyle\sum_{t=1}^T w_t \hat{f}_t(\boldsymbol{x}). \qquad(4.12)$$

弱学習器 $f_t$ は、弱学習器 $f_1,\ldots,f_{t-1}$ が判断を誤った点 $\boldsymbol{x}$ を重点的に学習する。点 $\boldsymbol{x}$ を選ぶ確率分布 $q_t(\boldsymbol{x})$ を検討しよう。
分類問題を想定し、式 4.13に示す**指数誤差**を最小化する。指数誤差の最小化は、関数 $\boldsymbol{f},\hat{\boldsymbol{f}}$ の値の内積の最大化である。

$$E(\hat{\boldsymbol{f}}) =
\exp \left\lbrace -{}^t\boldsymbol{f}(\boldsymbol{x}) \hat{\boldsymbol{f}}(\boldsymbol{x})\right\rbrace  =
\exp \left\lbrace -{}^t\boldsymbol{f}(\boldsymbol{x}) \displaystyle\sum_{t=1}^T w_t \hat{\boldsymbol{f}}_t(\boldsymbol{x})\right\rbrace  \geq 0. \qquad(4.13)$$

次に、特に分類問題を想定し、関数 $\hat{\boldsymbol{f}}$ が取り得る値に制約条件を設定する。関数 $\boldsymbol{f},\hat{\boldsymbol{f}}$ の値は、式 4.14の条件を満たす。

$$\displaystyle\sum_{k=1}^K f(\boldsymbol{x},k) =
\displaystyle\sum_{k=1}^K \hat{f}(\boldsymbol{x},k) = 1,
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\boldsymbol{f}(\boldsymbol{x}) =
\begin{bmatrix}
f(\boldsymbol{x},k)
\end{bmatrix}_k,\\
\hat{\boldsymbol{f}}(\boldsymbol{x}) =
\begin{bmatrix}
\hat{f}(\boldsymbol{x},k)
\end{bmatrix}_k.
\end{aligned}
\right. \qquad(4.14)$$

具体的には、真の関数 $\boldsymbol{f}$ は、式 4.15に示す値を取る。ただし、関数 $\hat{\boldsymbol{f}}$ は、式 4.14を満たす範囲で、自由な値を取る。

$$f(\boldsymbol{x},k) =
\begin{cases}
1& \text{if \(y=k\)},\\
\displaystyle\frac{1}{1-K}& \text{if \(y\neq k\)}.
\end{cases} \qquad(4.15)$$

式 4.13を分解すると、式 4.16を得る。この関数 $q_T$ を確率分布として、弱学習器 $f_T$ が学習する集合を無作為に選ぶ。

$$E(\hat{\boldsymbol{f}}) =
q_T(\boldsymbol{x}) \exp \left\lbrace -\boldsymbol{f}(\boldsymbol{x}) w_T \hat{\boldsymbol{f}}_T(\boldsymbol{x})\right\rbrace ,
\enspace\mathrm{where}\enspace
q_T(\boldsymbol{x}) =
\exp \left\lbrace -\boldsymbol{f}(\boldsymbol{x}) \displaystyle\sum_{t=1}^{T-1} w_t \hat{\boldsymbol{f}}_t(\boldsymbol{x})\right\rbrace . \qquad(4.16)$$

弱学習器 $f_T$ に対し、指数誤差 $E$ を最小化する加重 $w_T$ は、式 4.17で計算できる。**ラグランジュの未定乗数法**を使った。

$$\hat{w}_T = \log \left(\displaystyle\frac{1-E_T}{E_T}\right) + \log (K-1),
\enspace\mathrm{where}\enspace
E_T = q_T(\boldsymbol{x},y) \mathbb{I}(\hat{f}_T(\boldsymbol{x}) \neq y). \qquad(4.17)$$

以上の手法は、*AdaBoost*と呼ばれる。他にも、弱学習器の誤差を別の弱学習器で補う、**勾配ブースティング**が存在する。

### 4.5 汎化性能の最適化

第4.3節の議論を踏まえ、*AdaBoost*を実装しよう。基本的には、弱学習器を逐次的に作成し、追加する操作を繰り返す。

```scala
case class AdaBoost[T](x: Seq[(Seq[Int], T)], m: Int) extends Node[T] {
  val k = x.map(_._2).toSet
  val t = Seq(AdaStage(x, Seq.fill(x.size)(1.0 / x.size), m)).toBuffer
  def apply(x: Seq[Int]) = k.maxBy(y => t.map(_.score(x, y)).sum)
  while(t.last.best.error < 0.5) t += AdaStage(x, t.last.next, m)
}
```

弱学習器 $f_t$ は、 $M$ 個の候補 $\hat{f}_{tm}$ を作成し、最も高精度な候補を $f_t$ とする。この操作を $E_t$ が $0.5$ を超えるまで繰り返す。

```scala
case class AdaStage[T](x: Seq[(Seq[Int], T)], p: Seq[Double], m: Int) extends Node[T] {
  val best = List.fill(m)(Resample(x, p.map(_ / p.sum))).minBy(_.error)
  val gain = math.log((1 / best.error - 1) * (x.map(_._2).toSet.size - 1))
  val next = x.map(score).map(gain - _).map(math.exp).zip(p).map(_ * _)
  def score(x: Seq[Int], y: T) = if(this(x) == y) gain else 0
  def apply(x: Seq[Int]) = best(x)
}
```

弱学習器の候補 $\hat{f}_{tm}$ は、確率分布 $q_t$ に従う部分集合 $\mathbb{T}_t$ を学習する。集合 $\mathbb{T}_t$ は**ノイマンの棄却法**で擬似的に抽出できる。

```scala
case class Resample[T](x: Seq[(Seq[Int], T)], p: Seq[Double]) extends Node[T] {
  val data = Seq[(Seq[Int], T)]().toBuffer
  def reject(i: Int) = if(util.Random.nextDouble * p.max < p(i)) x(i) else null
  while(data.size < p.size) data += reject(util.Random.nextInt(p.size)) -= null
  def error = x.map((x, y) => this(x) != y).zip(p).filter(_._1).map(_._2).sum
  def apply(x: Seq[Int]) = quest(x)
  val quest = Question(data.toList)
}
```

Fig. 4.2は、各々が正規分布に従う3クラスの事例を学習した結果である。比較のため、Fig. 4.1と同じ事例を使用した。

![images/id3.bag50.png](/images/id3.bag50.png)

(1) Bagging  class.

![images/id3.ada50.png](/images/id3.ada50.png)

(2) AdaBoost class.

Fig. 4.2 region segmentation by ensemble learning.

決定木は、表現能力が過剰なので、学習不足を補うブースティングより、過学習を抑えるバギングの方が効果的である。

### 4.6 圧縮アルゴリズム

第4.1節で学んだ情報源符号化は、機械学習よりも、可逆圧縮の理論として知られる。その代表例が**ハフマン符号**である。
決定木に似た**ハフマン木**を作り、その分岐に2進数の符号語を割り当て、再帰的に圧縮と復元を行う。以下に実装を示す。

```scala
abstract class Node(val s: String, val w: Int) {
  def decode(bits: String, root: Node = this): String
  def encode(text: String, root: Node = this): String
}
```

引数は、この頂点が表す文字と、文字が現れる頻度である。また、文字列を受け取り、圧縮と復元を行う機能を実装する。
以下に、末端の頂点を実装する。復元の際は、頂点に対応する文字を出力し、圧縮の際は、何もせず最上位の頂点に戻る。

```scala
case class Atom(ch: String, freq: Int) extends Node(ch, freq) {
  def decode(bits: String, root: Node) = if(bits.isEmpty) ch else ch ++ root.decode(bits, root)
  def encode(text: String, root: Node) = if(text.size < 2) "" else root.encode(text.tail, root)
}
```

次に、符号語を表す頂点を実装する。厳密には、頂点と符号語の1桁を紐付けた頂点で、圧縮の際に、その桁を出力する。

```scala
case class Code(node: Node, bit: String) extends Node(node.s, node.w) {
  def decode(bits: String, root: Node) = node.decode(bits.tail, root)
  def encode(text: String, root: Node) = bit++node.encode(text, root)
}
```

分岐も実装する。復元の際は、符号語の $0,1$ に対応する頂点を、圧縮の際は、圧縮する文字を含む頂点を選び、巡回する。

```scala
case class Fork(nodes: Seq[Code]) extends Node(nodes.map(_.s).mkString, nodes.map(_.w).sum) {
  def decode(bits: String, root: Node) = nodes.find(_.bit.head == bits.head).get.decode(bits, root)
  def encode(text: String, root: Node) = nodes.find(_.s.contains(text.head)).get.encode(text, root)
}
```

次に、再帰的に木構造を構築する手順を実装する。まず、頻度が最低の部分木の組を選び、その親となる分岐を構築する。
また、部分木の頻度を合計し、新たな部分木の頻度とする。この操作を逐次的に繰り返し、完全なハフマン木を構築する。

```scala
implicit class Huffman(nodes: Seq[Node]) {
  def fork: Seq[Code] = nodes.zipWithIndex.map(_->_.toString).map(Code(_,_))
  def join: Seq[Node] = Seq(Fork(nodes.take(2).fork)).union(nodes.tail.tail)
  def tree: Seq[Node] = if(nodes.size <= 1) nodes else join.sortBy(_.w).tree
}
```

最後に、暗黙の型変換を利用して、文字列からハフマン木を生成する機能も実装した。この文字列が、圧縮の対象となる。

```scala
implicit class Symbols(source: String) {
  def countFreq = source.split("").groupBy(identity).mapValues(_.size)
  def toHuffman = countFreq.toSeq.map(Atom(_,_)).sortBy(_.w).tree.head
}
```

以上で、可逆圧縮が完成した。以下に、使用例を示す。なお、未知の文字を圧縮すると、例外が発生する点に、注意する。

```scala
val encoded = "Lorem ipsum dolor sit amet consectetur adipiscing elit".toHuffman.encode("lorem")
val decoded = "Lorem ipsum dolor sit amet consectetur adipiscing elit".toHuffman.decode(encoded)
println(encoded)
println(decoded)
```

以下に、出力を示す。文字が出現する頻度の偏りに起因して、平均情報量が抑制されたため、半分の圧縮率を達成できた。

```bash
110011110010100011111
lorem
```

