---
title: 3 サポートベクターマシン
---
## 3 サポートベクターマシン

**サポートベクターマシン**は、分類問題に対し、各クラスの集団からの距離 $d$ が最大になる境界を学習する分類器である。
Fig. 3.1(1)に線型分離可能な問題の、(2)に線型分離が困難な問題の例を示す。まずは、線型分離可能な場合を解説する。

![images/svm.model.png](/images/svm.model.png)

(1) linear problem.

![images/svm.round.png](/images/svm.round.png)

(2) curved problem.

Fig. 3.1 a support vector machine.

Fig. 3.1(1)の分類器は、式 3.1に従って、説明変数 $\boldsymbol{x}$ に対し、目的変数 $y$ を推定する。 $\boldsymbol{w}$ は加重で、 $c$ は定数項である。

$$\hat{y} = \mathrm{sign}\left(\boldsymbol{w} \cdot \boldsymbol{x}+c\right) \in \left\lbrace 1,-1\right\rbrace . \qquad(3.1)$$

距離 $d$ の最適化は**制約付き最適化問題**であり、学習対象の集合 $\mathbb{T}$ に対して、式 3.2に示す制約条件を満たす必要がある。

$$\forall \left(\boldsymbol{x},y\right) \in \mathbb{T} \colon y(\boldsymbol{w} \cdot \boldsymbol{x} + c) \geq 1,
\enspace\mathrm{where}\enspace
y\in\left\lbrace 1,-1\right\rbrace . \qquad(3.2)$$

距離 $d$ は式 3.3で求まる。式 3.2を念頭に、式を簡略化すると、距離 $d$ の最大化は加重 $\boldsymbol{w}$ の最小化と等価だと言える。

$$d(\mathbb{T}) = \min\displaystyle\frac{\left|\boldsymbol{w} \cdot \boldsymbol{x} + c\right|}{\left\|\boldsymbol{w}\right\|} = \displaystyle\frac{1}{\left\|\boldsymbol{w}\right\|},
\enspace\mathrm{where}\enspace
\boldsymbol{x}\in\mathbb{T}. \qquad(3.3)$$

現実には、式 3.2の**ハードマージン**は、誤分類に対して過剰に敏感なので、式 3.4に示す**ソフトマージン**を利用する。

$$\forall \left(\boldsymbol{x},y\right) \in \mathbb{T} \colon
y (\boldsymbol{w} \cdot \boldsymbol{x} + c) \geq 1 - \xi,
\enspace\mathrm{where}\enspace
\xi =
\begin{cases}
0 & \text{if \(y(\boldsymbol{w} \cdot \boldsymbol{x} + c) > 1\)}, \\
\left|y - (\boldsymbol{w} \cdot \boldsymbol{x} + c)\right| \geq 0 & \text{if \(y(\boldsymbol{w} \cdot \boldsymbol{x} + c) \leq 1\)}.
\end{cases} \qquad(3.4)$$

式 3.4は、誤分類された点 $\boldsymbol{x}$ に対し、罰を与える役割がある。 $\xi$ を**ヒンジ損失**と呼ぶ。最終的に式 3.5を最小化する。

$$f(\boldsymbol{w}) = C \displaystyle\sum_n^N \xi_n + \displaystyle\frac{1}{2}\left\|\boldsymbol{w}\right\|^2,
\enspace\mathrm{where}\enspace
C>0. \qquad(3.5)$$

定数 $C$ は、誤分類の許容量を決定する。小さな値に設定すると誤分類に鈍感になり、大きな値に設定すると敏感になる。

### 3.1 双対問題の導出

式 3.5は、式 3.4を束縛条件として、**ラグランジュの未定乗数法**で最小化できる。条件が2個ある点に注意を要する。

$$L(\boldsymbol{w},c,\xi,\lambda,\mu,\mathbb{T}) =
f(\boldsymbol{w}) - \displaystyle\sum_{i=1}^N \lambda_n \lbrace y_n(\boldsymbol{w} \cdot \boldsymbol{x}_n + c) - 1 + \xi_n\rbrace  - \displaystyle\sum_{i=1}^N \mu_n \xi_n. \qquad(3.6)$$

式 3.4の条件は不等式なので、式 3.7の**カルーシュ・クーン・タッカー条件**を満たす場合のみ、未定乗数法が使える。

$$\lambda_n \left\lbrace y_n\left(\boldsymbol{w} \cdot \boldsymbol{x}_n + c\right) - 1 + \xi_n\right\rbrace  = 0,\;
\left\lbrace 
\begin{aligned}
\lambda_n &\geq 0, \\
\mu_n     &\geq 0, \\
\mu_n\xi_n&=0.
\end{aligned}
\right. \qquad(3.7)$$

変数 $\lambda_n,\mu_n$ は未定乗数である。式 3.6を加重 $\boldsymbol{w}$ と定数 $c$ と未定乗数で偏微分すれば、 $L$ が極値になる条件が得られる。

$$\displaystyle\frac{\partial L}{\partial w} = \displaystyle\frac{\partial L}{\partial c} = \displaystyle\frac{\partial L}{\partial \lambda} = \displaystyle\frac{\partial L}{\partial \mu} = 0,
\Rightarrow
\left\lbrace 
\begin{aligned}
\boldsymbol{w}    &= \displaystyle\sum_{i=1}^N \lambda_n y_n \boldsymbol{x}_n, \\
0         &= \displaystyle\sum_{i=1}^N \lambda_n y_n, \\
\lambda_n &= C - \mu_n.\\
\end{aligned}
\right. \qquad(3.8)$$

Fig. 3.1(1)を振り返ると、 $C=0$ の場合は、式 3.7より、境界から距離 $d$ の点だけが $\lambda>0$ となり、加重 $\boldsymbol{w}$ に寄与する。
その点を**サポートベクトル**と呼ぶ。式 3.6に式 3.8を代入すると、都合よく $\xi$ や $C$ が消去され、式 3.9が得られる。

$$\tilde{L}(\lambda) =
\min_{\boldsymbol{w},c} L(\boldsymbol{w},c, \lambda) =
\displaystyle\sum_{i=1}^N \lambda_i \left\lbrace 1 - \displaystyle\frac{1}{2} \displaystyle\sum_{j=1}^N \lambda_j y_i y_j (\boldsymbol{x}_i \cdot \boldsymbol{x}_j)\right\rbrace  \leq f(\boldsymbol{w}). \qquad(3.9)$$

式 3.9の $\tilde{L}$ の最大化を $f(\boldsymbol{w})$ の**ラグランジュ双対問題**と呼ぶ。 $\tilde{L}$ と $f(\boldsymbol{w})$ の最適値は合致する。これを**強双対性**と呼ぶ。

### 3.2 逐次的な最適化

式 3.9の解析的な最適化は困難なため、**逐次最小問題最適化法**での最適化を検討する。まず、適当な2点 $\boldsymbol{x}_i,\boldsymbol{x}_j$ を選ぶ。
その2点の乗数 $\lambda_i,\lambda_j$ を式 3.10を満たす範囲で最適化する。以上の操作を、全ての点が式 3.7を満たすまで繰り返す。

$$y_i \delta_i + y_j \delta_j = 0,
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\delta_i &= \hat{\lambda}_i - \lambda_i \\
\delta_j &= \hat{\lambda}_j - \lambda_j
\end{aligned}
\right\rbrace 
\Leftarrow 0 = \displaystyle\sum_{i=1}^N \lambda_i y_i. \qquad(3.10)$$

2点 $\boldsymbol{x}_i,\boldsymbol{x}_j$ に対し、 $\tilde{L}$ の極大値を求める。式 3.10に注意して、式 3.9を $\delta_i,\delta_j$ で偏微分すると、式 3.11が得られる。

$$\displaystyle\frac{\partial \tilde{L}}{\partial \delta_i} = y_i (y_i-y_j)-\delta_i \left|\boldsymbol{x}_i-\boldsymbol{x}_j\right|^2-y_i \displaystyle\sum_{n=1}^N \lambda_n y_n \boldsymbol{x}_n \cdot (\boldsymbol{x}_i-\boldsymbol{x}_j). \qquad(3.11)$$

乗数 $\lambda_i,\lambda_j$ の移動量は式 3.12となる。ただし、式 3.7を満たす必要があり、 $0\leq\lambda\leq C$ の範囲で**クリッピング**を行う。

$$\delta_i = -\displaystyle\frac{y_i}{\left|\boldsymbol{x}_i-\boldsymbol{x}_j\right|^2} \left\lbrace \displaystyle\sum_{n=1}^N \lambda_n y_n \boldsymbol{x}_n \cdot (\boldsymbol{x}_i-\boldsymbol{x}_j)-y_i+y_j\right\rbrace . \qquad(3.12)$$

なお、定数 $c$ の値は、 $y(\boldsymbol{w} \cdot \boldsymbol{x})$ を最小化する点 $\boldsymbol{x}$ に着目すると、式 3.13で計算できる。以上で、必要な数式が出揃った。

$$c = -\displaystyle\frac{1}{2} \left\lbrace 
\min_{i|y_i=+1} \displaystyle\sum_{j=1}^N \lambda_j y_j \boldsymbol{x}_i \cdot \boldsymbol{x}_j +
\max_{j|y_j=-1} \displaystyle\sum_{i=1}^N \lambda_i y_i \boldsymbol{x}_j \cdot \boldsymbol{x}_j\right\rbrace . \qquad(3.13)$$

逐次最小問題最適化法の最悪計算時間は $O(n^3)$ だが、点 $\boldsymbol{x}_i$ を選ぶ際に、式 3.7に反する点を重視すると効率的である。

### 3.3 線型分離の学習

第3.2節までの議論に基づき、逐次最小問題最適化法を実装する。まず、組 $\left(\boldsymbol{x},y\right)$ を実装する。乗数 $\lambda$ を変数として持つ。

```scala
case class Data(x: Seq[Double], t: Int, var l: Double = 0) {
  def kkt(svm: SVM, C: Double) = t * svm(this) match {
    case e if e < 1 => l >= C
    case e if e > 1 => l == 0
    case _ => true
  }
}
```

次に、サポートベクターマシンの本体を実装する。引数kは内積である。敢えて抽象化したのは、第3.4節の布石である。

```scala
class SVM(data: Seq[Data], k: (Data, Data) => Double) {
  var const = 0.0
  def group(t: Int) = data.filter(_.t == t).map(apply)
  def apply(x: Data) = data.map(d => d.l * d.t * k(x,d)).sum + const
}
```

最後に、逐次最小問題最適化法を実装する。第3.2節に述べた数式を実装し、式 3.7を満たすまで逐次的に最適化する。

```scala
class SMO(data: Seq[Data], k: (Data, Data) => Double, C: Double = 1e-10) extends SVM(data,k) {
  while(data.filterNot(_.kkt(this,C)).size >= 2) {
    val a = data(util.Random.nextInt(data.size))
    val b = data(util.Random.nextInt(data.size))
    val min = math.max(-a.l, if(a.t == b.t) b.l - this.C else -b.l)
    val max = math.min(-a.l, if(a.t == b.t) b.l - this.C else -b.l) + C
    val prod = this(Data(a.x.zip(b.x).map(_-_), 0)) - this.const
    val best = -a.t * (prod - a.t + b.t) / (k(a,a) - 2 * k(a,b) + k(b,b))
    if(!best.isNaN) a.l += a.t * a.t * math.max(min, math.min(max, best))
    if(!best.isNaN) b.l -= a.t * b.t * math.max(min, math.min(max, best))
    this.const = -0.5 * (group(+1).min + group(-1).max) + this.const
  }
}
```

Fig. 3.2に学習の例を示す。綺麗な境界を学習できた。(2)では、誤分類によりサポートベクトルが消える様子がわかる。

![images/svm.line1.png](/images/svm.line1.png)

(1) sample data separable by a line.

![images/svm.line2.png](/images/svm.line2.png)

(2) sample data with outlier points.

Fig. 3.2 decision surface learned by a linear SVM.

黒の点線はクラスの境界を表し、赤と青の点線はサポートベクトルを表す。赤と青の濃淡は $\boldsymbol{w} \cdot \boldsymbol{x}+c$ の値の勾配を表す。

### 3.4 特徴空間の変換

第3.2節までの議論は、線型分離可能な問題が前提だった。第3.4節では、線型分離が困難な問題に議論の対象を拡げる。
線型分離が困難な問題でも、非線型の適当な関数 $\Phi$ で他の空間に写像し、線型分離可能な問題に変換できる場合がある。

$$\Phi: \boldsymbol{x} \mapsto \Phi_{\boldsymbol{x}}. \qquad(3.14)$$

具体例を挙げると、式 3.15の写像 $\Phi_g$ は、点 $\boldsymbol{x}$ を無限の次元を持つ点 $\Phi_{\boldsymbol{x}}$ に変換する、無限次元の空間への写像である。
低次元の空間では、点 $\boldsymbol{x}$ を線型分離するのが困難でも、無限次元に引き延ばせば、必ず適当な超平面で線型分離できる。

$$\Phi_g(\boldsymbol{x}) = \exp \left(- \displaystyle\frac{1}{2\sigma^2} \left\|\boldsymbol{x}\right\|^2\right)
\begin{bmatrix}
\displaystyle\frac{1}{\sqrt{n!}} \displaystyle\frac{x_d^n}{\sigma^n}
\end{bmatrix}_{dn},
\enspace\mathrm{where}\enspace
n = 0,1,2,\ldots,\infty. \qquad(3.15)$$

第3.2節までの議論を振り返ると、内積 $\boldsymbol{x}_i \cdot \boldsymbol{x}_j$ が何度も現れた。第3.4節では写像 $\Phi$ を通すので、 $\Phi_{\boldsymbol{x}_{i}} \cdot \Phi_{\boldsymbol{x}_{j}}$ の形になる。
無限次元の内積の計算量は無限で、写像 $\Phi$ の計算も困難である。しかし、**テイラー級数**を使えば、簡単に内積が求まる。

$$\Phi_{\boldsymbol{x}_{i}} \cdot \Phi_{\boldsymbol{x}_{j}} =
\exp \left\lbrace - \displaystyle\frac{1}{2\sigma^2} \left\|\boldsymbol{x}_i-\boldsymbol{x}_j\right\|^2\right\rbrace 
\Leftarrow
e^x = \displaystyle\sum_{n=0}^\infty \displaystyle\frac{x^n}{n!}. \qquad(3.16)$$

内積を計算可能な写像 $\Phi$ を使うことで、陽に $\Phi$ を計算せずに、仮想的な高次元空間に写像する技法を**カーネル法**と呼ぶ。
理論的には、**正定値性**を満たす対称関数 $k$ に対し、内積が $k$ で定義される**再生核ヒルベルト空間**への写像 $\Phi$ が存在する。

$$k \colon \mathbb{M} \times \mathbb{M} \to \mathbb{R},
\enspace\mathrm{where}\enspace
k(\boldsymbol{x}_i,\boldsymbol{x}_j) = k(\boldsymbol{x}_j,\boldsymbol{x}_i). \qquad(3.17)$$

関数 $k$ が正定値性を満たすとは、点 $\boldsymbol{x}$ を元に持つ空間 $\Omega$ に対し、式 3.18の**グラム行列**が正定値行列である場合を指す。

$$\begin{bmatrix}
k(\boldsymbol{x}_i,\boldsymbol{x}_j)
\end{bmatrix}_{ij},
\enspace\mathrm{where}\enspace
\boldsymbol{x}_i,\boldsymbol{x}_j \in \Omega. \qquad(3.18)$$

関数 $k$ を利用して空間 $\Omega$ を空間 $H_k$ に写像すると、空間 $H_k$ の元 $f,g$ の内積 $\left\langle f,g\right\rangle$ は、**再生性**により関数 $k$ で定義できる。

$$\forall a_i,b_j \in \mathbb{R} \colon
\left\langle f,g\right\rangle =
\left\langle \displaystyle\sum_{i=1}^N a_i k(\boldsymbol{x},\boldsymbol{x}_i),\displaystyle\sum_{j=1}^N b_j k(\boldsymbol{x},\boldsymbol{x}_j)\right\rangle =
\displaystyle\sum_{i=1}^N \displaystyle\sum_{j=1}^N a_i b_j k(\boldsymbol{x}_i, \boldsymbol{x}_j). \qquad(3.19)$$

要するに、正定値性を満たす任意の対称関数 $k$ に対し、内積が関数 $k$ で定義された空間 $H_k$ が存在し、内積を計算できる。
最も汎用的な例は、式 3.16の**ガウシアンカーネル**である。Fig. 3.3に、線型分離が困難な問題を学習した結果を示す。

![images/svm.kern1.png](/images/svm.kern1.png)

(1) diamond-shaped samples.

![images/svm.kern2.png](/images/svm.kern2.png)

(2) two concentric circles.

Fig. 3.3 decision surface learned by a Gaussian SVM.

第3.2節に掲載した実装に、適当な内積の定義を与えれば、任意の写像を試せる。手作りで、無限次元の魔法を味わおう。

### 3.5 ヒルベルト空間

第3.4節は、内積と距離が式 3.17の関数 $k$ で定義され、無限級数の極限も計算可能な空間 $H_k$ が存在する点に依拠する。
空間 $H$ が線型空間で、式 3.20を満たす関数 $\left\langle \boldsymbol{x},\boldsymbol{y}\right\rangle$ が存在する場合に、これを内積と呼び、空間 $H$ を**内積空間**と呼ぶ。

$$\forall a_i,b_j \in \mathbb{R} \colon
\left\langle \displaystyle\sum_{i=1}^I a_i \boldsymbol{x}_i,\displaystyle\sum_{j=1}^J b_j \boldsymbol{y}_j\right\rangle =
\left\langle \displaystyle\sum_{j=1}^J b_j \boldsymbol{y}_j,\displaystyle\sum_{i=1}^I a_i \boldsymbol{x}_i\right\rangle =
\displaystyle\sum_{i=1}^I \displaystyle\sum_{j=1}^J a_i b_j \left\langle \boldsymbol{x}_i,\boldsymbol{y}_j\right\rangle,\;
\left\langle \boldsymbol{x},\boldsymbol{x}\right\rangle \geq 0. \qquad(3.20)$$

高校数学で学ぶ標準内積は、この定義に従う。また、関数 $f,g$ を元とする空間では、その内積は式 3.21で定義できる。

$$\left\langle f,g\right\rangle = \int_H f(\boldsymbol{x}) \overline{g(\boldsymbol{x})} d\mu(\boldsymbol{x}). \qquad(3.21)$$

関数 $\mu$ は関数空間 $H$ の**測度**である。さて、内積空間 $H$ では、式 3.22に示す通り、内積を使って**ノルム**を定義できる。

$$\left\|\boldsymbol{x}\right\| = \left\langle \boldsymbol{x},\boldsymbol{x}\right\rangle^{\frac{1}{2}} \in \mathbb{R}. \qquad(3.22)$$

式 3.22を利用して、任意の2点の距離 $d$ を定義できる。距離 $d$ が式 1.2を満たす場合に、空間 $H$ は**距離空間**である。

$$d(\boldsymbol{x},\boldsymbol{y}) = \left\|\boldsymbol{x}-\boldsymbol{y}\right\| \geq 0. \qquad(3.23)$$

空間 $H$ で定義された任意の級数が、空間 $H$ の元に収束する場合に、空間 $H$ は**完備性**を満たし、**ヒルベルト空間**となる。
正定値性を満たす適当な対称関数 $k$ を定義して、関数 $k$ の線型結合で式 3.24の空間 $H_0$ を作る。これを**線型包**と呼ぶ。

$$H_0 = \mathrm{span}\left\lbrace \displaystyle\sum_{n=1}^n a_n k(\boldsymbol{x}_n,\cdot)|a_n \in \mathbb{R}\right\rbrace . \qquad(3.24)$$

空間 $H_0$ の元 $f,g$ に対し、内積を式 3.25の通りに定義する。証明は省くが、空間 $H_0$ はヒルベルト空間の条件を満たす。

$$\left\langle f,g\right\rangle_{H_0} = \displaystyle\sum_{i=1}^I \displaystyle\sum_{j=1}^J a_i b_j k(\boldsymbol{x}_i,\boldsymbol{x}_j),
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
f(\boldsymbol{x}) &= \displaystyle\sum_{i=1}^I a_i k(\boldsymbol{x}_i,\boldsymbol{x}),\\
g(\boldsymbol{x}) &= \displaystyle\sum_{j=1}^J b_j k(\boldsymbol{x}_j,\boldsymbol{x}).
\end{aligned}
\right. \qquad(3.25)$$

ぜひ、式 3.25が式 3.20の性質を満たし、その内積で距離 $d$ を定義すると、式 1.2の公理を満たす点を確認しよう。
さて、式 3.25より自明だが、空間 $H_0$ の元 $f$ は、式 3.26の再生性を満たし、空間 $H_0$ は**再生核ヒルベルト空間**となる。

$$f(\boldsymbol{x}) = \displaystyle\sum_{n=1}^N a_n k(\boldsymbol{x}_n,\boldsymbol{x}) = \left\langle f,k(\cdot,\boldsymbol{x})\right\rangle_{H_0}. \qquad(3.26)$$

再生性を持つ関数 $k$ を**再生核**と呼ぶ。核は式 3.27に示す**積分変換**に由来する。これは、空間 $\Omega_s,\Omega_t$ の間の写像である。

$$F(\boldsymbol{s}) = \int_{\Omega_t} k(\boldsymbol{s}, \boldsymbol{t}) f(\boldsymbol{t}) d\boldsymbol{t},
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\boldsymbol{s} &\in \Omega_s,\\
\boldsymbol{t} &\in \Omega_t.
\end{aligned}
\right. \qquad(3.27)$$

例えば、**ラプラス変換**や**フーリエ変換**が該当する。さて、式 3.28で定義される核関数 $k$ は、再生核である。証明しよう。

$$k(x,y) = \displaystyle\frac{a}{\pi} \mathrm{sinc} a(y-x). \qquad(3.28)$$

式 3.21に従って内積を求めると、式 3.29を得る。式 3.28が矩形関数の双対である点に注目して、再生性を導ける。

$$\left\langle f,k(x,\cdot)\right\rangle_{L^2} =
\int_{-\infty}^\infty f(y) \overline{k(x,y)} dy =
\displaystyle\frac{1}{2\pi} \int_{-a}^a \mathcal{F}_f(\omega) e^{i \omega x} d\omega =
f(x). \qquad(3.29)$$

積分変換と機械学習の関係は興味深く、特に、深層学習の優れた性能の理由を積分変換に求める研究は、注目に値する。
簡単な例では、式 3.30に示す**シグモイドカーネル**の挙動は、式 2.1で加重 $\boldsymbol{w}$ が固定されたニューロンと等価になる。

$$k(\boldsymbol{w},\boldsymbol{x}) = \tanh {}^t\boldsymbol{w} \boldsymbol{x}. \qquad(3.30)$$

深層学習は、勾配法を通じて加重 $\boldsymbol{w}$ を最適化するため、自在に最適化される高次元空間の層を持つのと等価だと言える。

