---
title: 2 ニューラルネットワーク
---
## 2 ニューラルネットワーク

**ニューラルネットワーク**は、線型回帰に似た**ニューロン**と呼ばれる関数を連結して、連鎖構造にした複雑な関数である。
単体のニューロンは、線型回帰の後に、**活性化関数**と呼ばれる非線型な関数 $f$ を適用した関数で、式 2.1で定義される。

$$\boldsymbol{y} \simeq f(\boldsymbol{z}) = f(W\boldsymbol{x}). \qquad(2.1)$$

単体では、線型回帰と同じ程度の表現能力だが、何層も重ねることで、任意の**滑らかな関数**を任意の精度で近似できる。
循環構造がなく、直線的な構造の**順伝播型**の動作は、式 2.2の漸化式で定義できる。循環構造の場合は第2.5節で扱う。

$$\boldsymbol{y}_n = \boldsymbol{x}_{n+1} = f_n(\boldsymbol{z}_n) = f_n(W_n\boldsymbol{x}_n). \qquad(2.2)$$

式 2.2で、第 $n$ 層は前の層から値 $\boldsymbol{x}_n$ を受容し、行列 $W_n$ で加重して活性化関数 $f_n$ を適用し、後続の層に値 $\boldsymbol{y}_n$ を渡す。
活性化関数には、**シグモイド関数**が広く利用される。式 2.3に定義する。これは、2クラスの分類器のように振る舞う。

$$f_\mathrm{sigm}(z) = \displaystyle\frac{1}{1 + e^{-z}} = \displaystyle\frac{1}{2}\tanh\displaystyle\frac{z}{2} + \displaystyle\frac{1}{2}. \qquad(2.3)$$

代表的な活性化関数の例をFig. 2.1(1)に示す。他には、式 2.4に示す**ソフトマックス関数**も特に最終層で利用される。

$$y \sim \hat{p}\left(y\right) = f_\mathrm{smax}(\boldsymbol{z}) =
\displaystyle\frac{1}{e^{z_1}+\cdots+e^{z_K}}
\begin{pmatrix}
e^{z_1}\\
\vdots\\
e^{z_K}
\end{pmatrix}. \qquad(2.4)$$

最終層の活性化関数を適切に選ぶと、回帰や分類など、様々な問題に対応できる。特に分類問題の例は第2.3節に述べる。
Fig. 2.1(2)は、活性化関数にシグモイド関数を利用し、論理和を求める例である。直線 $f(\boldsymbol{x})=0.5$ は分類の境界を表す。

![images/slp.trans.png](/images/slp.trans.png)

(1) activation functions.

![images/slp.class.png](/images/slp.class.png)

(2) OR learned by neuron.

Fig. 2.1 neuron mechanism.

論理和や論理積は、分類の境界が直線や超平面となる単純な問題で、これを**線型分離可能**と呼び、単層でも表現できる。
第2章で学ぶ誤差逆伝搬法は、多数の層を訓練して、線型分離が困難な問題に適合させる**深層学習**を支える技法である。

### 2.1 誤差逆伝播法の理論

深層学習では、多数の層の加重を最適化して、誤差 $E$ を最小化する。最適解の計算は困難なので、逐次的に最適化する。
具体的な手順は、以下の通りである。まず、第 $n$ 層の加重 $W_n$ を最適化の対象とし、式 2.5に示す勾配法で最適化する。

$$w_n'^{ij}
= w_n^{ij}-\eta \displaystyle\frac{\partial E}{\partial w_n^{ij}}
= w_n^{ij}-\eta \displaystyle\frac{\partial z_n^j}{\partial w_n^{ij}} \displaystyle\frac{\partial x_{n+1}^j}{\partial z_n^j} \displaystyle\frac{\partial E}{\partial x_{n+1}^j}
= w_n^{ij}-\eta x_n^i \displaystyle\frac{\partial f}{\partial z_n^j}(z_n^j) \displaystyle\frac{\partial E}{\partial x_{n+1}^j}. \qquad(2.5)$$

定数 $\eta$ は学習率で、変数 $x_n^i,z_n^j$ は、変数 $\boldsymbol{x}_n,\boldsymbol{z}_n$ の第 $i,j$ 成分である。さて、式 2.2から式 2.6の漸化式が導出される。
式 2.6の漸化式を利用して、誤差 $E$ を逆方向に伝播させ、式 2.5の最適化を各層で行う。これを**誤差逆伝播法**と呼ぶ。

$$\displaystyle\frac{\partial E}{\partial x_n^i}
= \displaystyle\sum_{j=1}^J \displaystyle\frac{\partial z_n^j}{\partial x_n^i} \displaystyle\frac{\partial x_{n+1}^j}{\partial z_n^j} \displaystyle\frac{\partial E}{\partial x_{n+1}^j}
= \displaystyle\sum_{j=1}^J w_n^{ij} \displaystyle\frac{\partial f}{\partial z_n^j}(z_n^j) \displaystyle\frac{\partial E}{\partial x_{n+1}^j}. \qquad(2.6)$$

漸化式の初期値を考える。2乗誤差関数 $E_\mathrm{sq}$ を仮定すると、最終層の値 $\hat{\boldsymbol{y}}$ と目的変数 $\boldsymbol{y}$ に対し、導関数は式 2.7になる。

$$\displaystyle\frac{\partial E_\mathrm{sq}}{\partial \hat{y}^j} = \displaystyle\frac{\partial }{\partial \hat{y}^j} \displaystyle\frac{1}{2} \left\|\boldsymbol{y}-\hat{\boldsymbol{y}}\right\|^2,
\enspace\mathrm{where}\enspace
E_\mathrm{sq}(\hat{\boldsymbol{y}},\boldsymbol{y}) = \displaystyle\frac{1}{2} \left\|\boldsymbol{y}-\hat{\boldsymbol{y}}\right\|^2. \qquad(2.7)$$

式 2.6には、活性化関数の微分が含まれるが、活性化関数は巧妙に設計されており、実に単純な四則演算で計算できる。

$$\displaystyle\frac{\partial f_\mathrm{sigm}}{\partial z_n^j}(z_n^j) = \displaystyle\frac{e^{-z_n^j}}{(1+e^{-z_n^j})^2} = x_{n+1}^j(1-x_{n+1}^j). \qquad(2.8)$$

第2.2節では、誤差逆伝播法を備えると同時に、自在に層構造を定義可能な深層学習を実装する。利用方法を以下に示す。

```scala
val model3 = new Output(1, _-_)
val model2 = new Offset(3, new Sigmoid, ()=>new PlainSGD, model3)
val model1 = new Offset(2, new Sigmoid, ()=>new PlainSGD, model2)
for(n <- 1 to 1000000; x <- 0 to 1; y <- 0 to 1) model1.bp(Seq(x,y), Seq(x^y))
```

複数の非線型変換を持つ恩恵で、線型分離が困難な分類問題にも対応できる。具体例として、排他的論理和を学習する。
通常の結果をFig. 2.2(1)に、各層の変数 $\boldsymbol{x}$ に定数項を含む場合の結果を(2)に示す。定数項の有無で、境界が変化した。

![images/mlp.class.png](/images/mlp.class.png)

(1) Hidden + Hidden.

![images/mlp.const.png](/images/mlp.const.png)

(2) Offset + Offset.

Fig. 2.2 exclusive OR learned by a three-layer perceptron.

ぜひ実装して、学習の途中経過を観察しよう。また、加重の初期値によって、収束までの時間が変わる様子も観察できる。

### 2.2 誤差逆伝播法の実装

以上の数式を実装し、誤差 $E$ を逆伝播させ、最終層から最初層まで逐次的に最適化しよう。最初に、勾配法を定義する。

```scala
abstract class SGD(var w: Double = math.random) extends (Double => Unit)
```

これは、加重 $w$ の抽象化であり、式 2.5による最適化も行う。具体的な最適化の手順は、勾配法を継承して実装する。

```scala
class PlainSGD(e: Double = 0.01) extends SGD {
  def apply(dE: Double): Unit = this.w -= e * dE
}
```

引数は、学習率 $\eta$ である。学習時は、誤差 $E$ の勾配 $\nabla E$ を受け取り、加重 $w$ を修正する。次に、活性化関数を定義する。

```scala
trait Act {
  def fp(z: Seq[Double]): Seq[Double]
  def bp(y: Seq[Double]): Seq[Double]
}
```

活性化関数は、順伝播と逆伝播を行う。以下に、具体的な実装例を示す。順伝播は式 2.3に、逆伝播は式 2.8に従う。

```scala
class Sigmoid extends Act {
  def fp(z: Seq[Double]) = z.map(z => 1 / (1 + math.exp(-z)))
  def bp(z: Seq[Double]) = this.fp(z).map(y => y * (1.0 - y))
}
```

次に、層を定義する。加重と活性化関数を持ち、順伝播と逆伝播を行う中間層と、誤差関数を計算する最終層が派生する。

```scala
abstract class Neuron(val dim: Int) {
  def fp(x: Seq[Double]): Seq[Double]
  def bp(x: Seq[Double], t: Seq[Double]): Seq[Double]
}
```

最終層を実装する。引数は、最終層が出力する値の要素数と、誤差の導関数である。誤差関数は、**損失関数**とも呼ばれる。

```scala
class Output(dim: Int = 1, loss: (Double,Double)=>Double = _-_) extends Neuron(dim) {
  def fp(x: Seq[Double]) = x
  def bp(x: Seq[Double], t: Seq[Double]) = x.zip(t).map(loss.tupled)
}
```

中間層も実装する。引数は、中間層が受け取る値の要素数と、活性化関数と、加重を生成する関数と、後続の層である。

```scala
class Hidden(dim: Int, act: Act, weight: ()=>SGD, next: Neuron) extends Neuron(dim) {
  lazy val w = List.fill(next.dim, dim)(weight())
  def fp(x: Seq[Double]) = next.fp(act.fp(wx(x)))
  def wx(x: Seq[Double]) = w.map(_.map(_.w).zip(x).map(_ * _).sum)
  def bp(x: Seq[Double], t: Seq[Double]) = ((z: Seq[Double]) => {
    val bp = next.bp(act.fp(z),t).zip(act.bp(z)).map(_ * _)
    for((w,g) <- w.zip(bp); (sgd,x) <- w.zip(x)) sgd(x * g)
    w.transpose.map(_.zip(bp).map(_.w * _).sum)
  })(wx(x))
}
```

最後に、定数項を実装する特殊な中間層も実装する。仕組みは単純で、中間層が受け取る値に定数 $1$ の要素を追加する。

```scala
class Offset(dim: Int, act: Act, weight: ()=>SGD, next: Neuron) extends Neuron(dim) {
  lazy val body = new Hidden(dim + 1, act, weight, next)
  def fp(x: Seq[Double]) = body.fp(x.padTo(dim + 1, 1d))
  def bp(x: Seq[Double], t: Seq[Double]) = body.bp(x.padTo(dim + 1, 1d), t).init
}
```

### 2.3 ソフトマックス関数

**多クラス分類**の問題では、最終層の活性化関数に式 2.4のソフトマックス関数とし、各クラスの確率分布 $p$ を学習する。
誤差関数には、式 2.9の**交差エントロピー**を使う。式 2.9の $H(p)$ は、確率分布 $p$ の不偏性を表す**平均情報量**である。

$$E_\mathrm{CE}(p,\hat{p}) =
-\int p\left(\boldsymbol{y}\right) \log \hat{p}\left(\boldsymbol{y}\right) d\boldsymbol{y} =
-\int p\left(\boldsymbol{y}\right) \left\lbrace \log p\left(\boldsymbol{y}\right) - \log\displaystyle\frac{p\left(\boldsymbol{y}\right)}{\hat{p}\left(\boldsymbol{y}\right)}\right\rbrace  d\boldsymbol{y} =
H(p) + D\!\left(p\|\hat{p}\right) \geq
D\!\left(p\|\hat{p}\right). \qquad(2.9)$$

式 2.10の $D\!\left(p\ \vert \hat{p}\right)$ を**カルバック・ライブラー情報量**と呼ぶ。これは非負で、確率分布 $p,\hat{p}$ が等価な場合に限り $0$ になる。
式 2.10から確率分布 $\hat{p}$ の項を抽出すると、分布 $\hat{p}$ の対数の期待値である。また、分布 $\hat{p}$ は各層の加重 $W_n$ の尤度である。

$$D\!\left(p\|\hat{p}\right) =
\int_K p\left(y\right) \log \displaystyle\frac{p\left(y\right)}{\hat{p}\left(y\right)} dy \geq \int_K p\left(y\right) \left(1-\displaystyle\frac{\hat{p}\left(y\right)}{p\left(y\right)}\right) dy = 0. \qquad(2.10)$$

分類器が推定する確率分布 $\hat{p}$ を真の確率分布 $p$ に近付けるには、式 2.9を最小化し、間接的に式 2.10を最小化する。
これは、尤度 $\hat{p}$ の対数の期待値の最大化に相当し、即ち最尤推定である。例えば、最終層では式 2.11の勾配法を行う。

$$\displaystyle\frac{\partial E_\mathrm{CE}}{\partial z^k}
= - \displaystyle\frac{\partial }{\partial z^k} \displaystyle\sum_{i=1}^K y^i \left(\log e^{z^i} - \log \displaystyle\sum_{j=1}^K e^{z^j}\right)
= - y^k + \displaystyle\sum_{i=1}^K y^i \hat{y}^k = -y^k + \hat{y}^k. \qquad(2.11)$$

以上の議論を踏まえ、式 2.4の順伝播と式 2.11の勾配計算を実装する。この活性化関数は、最終層でのみ使用できる。

```scala
class Softmax extends Act {
  def fp(z: Seq[Double]) = z.map(math.exp(_)/z.map(math.exp).sum)
  def bp(z: Seq[Double]) = Seq.fill(z.size)(1.0)
}
```

勾配 $\nabla f(\boldsymbol{z})$ の計算を誤魔化したので、中間層で使うと、逆伝播が妨害される。その点に目を瞑れば、簡単に実装できた。

```scala
val model = new Offset(3, new Softmax, ()=>new PlainSGD, new Output(3, _-_))
```

Fig. 2.3は、チェコ共和国の国旗を学習する例である。意匠が直線的なので、単層の方が多層より正確な旗を学習できる。

![images/slp.czech.png](/images/slp.czech.png)

(1) 2-layer perceptron

![images/mlp.czech.png](/images/mlp.czech.png)

(2) 3-layer perceptron

Fig. 2.3 Czech flag learned by a perceptron.

層を増やすと、表現能力は高まるが、第2.4節でも述べる通り、学習が停滞しやすく、却って精度が低下する場合がある。

### 2.4 鞍点と学習率の調整

勾配法には、最適解に到達する前に**鞍点**で最適化が停滞する場合がある。式 2.12に示す関数 $E$ の最小化の例で考える。
鞍点とは、ある方向では極大値だが、別の方向では極小値となる停留点である。関数 $E$ の場合は、原点 $\boldsymbol{O}$ が鞍点である。

$$\Delta E = \displaystyle\frac{\partial f}{\partial x} \Delta x + \displaystyle\frac{\partial f}{\partial y} \Delta y = 2x \Delta x - 2y \Delta y,
\enspace\mathrm{where}\enspace
E(x,y) = x^2 - y^2. \qquad(2.12)$$

原点 $\boldsymbol{O}$ に嵌ると、Fig. 2.4(1)のように最適化が止まる。しかし、 $y\neq0$ に動けば、勾配が負になり、最適化を再開できる。
鞍点は頻繁に現れる。Fig. 2.4(2)は、5通りの初期値で排他的論理和を学習した際の誤差 $E_\mathrm{sq}$ の推移で、停滞が見られる。

![images/sgd.avoid.png](/images/sgd.avoid.png)

(1) saddle point avoidance mechanism.

![images/sgd.speed.png](/images/sgd.speed.png)

(2) loss diminution through training.

Fig. 2.4 comparison of PlainSGD and AdaDelta.

対策として、最適解の近傍では学習率を小さく、鞍点の近傍で学習率を大きく調節する、適応的な勾配法が利用される。
例えば、式 2.13の*AdaGrad*は、時刻 $t$ での学習率を勾配 $\nabla E_t$ の期待値の逆数とし、また、時刻 $t$ に従って減衰させる。

$$\Delta w = -\displaystyle\frac{\eta}{t\sqrt{\underset{}{\mathbf{E}}\!\left[\,(\nabla E)^2\,\right]_t}},
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\underset{}{\mathbf{E}}\!\left[\,(\nabla E)^2\,\right]_t &= \displaystyle\frac{1}{t} \displaystyle\sum_{\tau=0}^t (\nabla E_\tau)^2, \\
\underset{}{\mathbf{E}}\!\left[\,(\nabla E)^2\,\right]_0 &= \varepsilon.
\end{aligned}
\right. \qquad(2.13)$$

*AdaGrad*は全時間の勾配を考慮するが、式 2.14の*AdaDelta*では、期待値に加重 $\rho$ を導入し、直近の勾配を重視する。
式 2.14の分数には、加重 $w$ と勾配 $\nabla E$ の単位を変換する役割がある。なお、定数 $\varepsilon$ はゼロ除算を防ぐ微小な値である。

$$\Delta w_{mt} = -\displaystyle\frac{\sqrt{\underset{}{\mathbf{E}}\!\left[\,(\Delta w)^2\,\right]_t+\varepsilon}}{\sqrt{\underset{}{\mathbf{E}}\!\left[\,(\nabla E)^2\,\right]_t+\varepsilon}} \nabla E_{mt},
\enspace\mathrm{where}\enspace
\left\lbrace 
\begin{aligned}
\underset{}{\mathbf{E}}\!\left[\,x\,\right]_t &= \rho \underset{}{\mathbf{E}}\!\left[\,x\,\right]_{t-1} + (1-\rho) x_t, \\
\underset{}{\mathbf{E}}\!\left[\,x\,\right]_0 &= 0.
\end{aligned}
\right. \qquad(2.14)$$

以上の議論を踏まえ、*AdaDelta*を実装する。引数は、定数 $\rho,\varepsilon$ である。Fig. 2.4に、単純な勾配法との性能の比較を示す。

```scala
class AdaDelta(r: Double = 0.95, e: Double = 1e-8) extends SGD {
  var eW, eE = 0.0
  def apply(dE: Double) = {
    lazy val v = math.sqrt(eW + e) / math.sqrt(eE + e)
    this.eE = r * eE + (1 - r) * math.pow(1 * dE, 2.0)
    this.eW = r * eW + (1 - r) * math.pow(v * dE, 2.0)
    this.w -= v * dE
  }
}
```

### 2.5 通時的誤差逆伝播法

自然言語や信号など、時系列の未来を予測するには、状態を記憶し、時系列を逐次的に順伝播させる機能が必要である。
**再帰型ニューラルネットワーク**が代表的で、中間層の状態 $\boldsymbol{y}$ を、次の時刻の入力 $\boldsymbol{x}$ の順伝播に合流させ、中間層に戻す。

$$\boldsymbol{y}^t = f(\boldsymbol{z}^t) = f(W_i\boldsymbol{x}^t+W_h\boldsymbol{y}^{t-1}). \qquad(2.15)$$

再帰構造を展開して、擬似的に再帰を除去すれば、従来通り勾配法で最適化できる。これを**通時的誤差逆伝播法**と呼ぶ。
以下に実装を示す。順伝播は、同じ中間層を繰り返し通過し、最後に、後続の層に伝播する。逆伝播も、同様に実装する。

```scala
class RNN(dim: Int, hidden: Neuron, output: Neuron, value: Double = 0) extends Neuron(dim) {
  val hist = Seq[Seq[Double]]().toBuffer
  val loop = Seq[Seq[Double]]().toBuffer
  def fp(x: Seq[Double]) = output.fp(hp(x).last)
  def tt(x: Seq[Double]) = hist.zip(loop).map(_++_).foldRight(x)
  def hp(x: Seq[Double]) = loop.append(hidden.fp(hist.append(x).last++loop.last))
  def bp(x: Seq[Double], t: Seq[Double]) = tt(output.bp(hp(x).last,t))(hidden.bp)
  def init = hist.clear -> loop.clear -> loop.append(Seq.fill(hidden.dim)(value))
}
```

順伝播や逆伝播を行う度に、中間層の状態が記憶される。従って、時系列の処理が終わる度に、初期化する必要がある。
勾配法も実装し直す。逆伝播の間に、中間層の挙動が変わると最適化に障るので、逆伝播の完了まで、加重を据え置く。

```scala
class DelaySGD(sgd: SGD = new PlainSGD, var d: Double = 0) extends SGD {
  def apply(dE: Double) = (sgd.w = w, sgd(dE), d += sgd.w - w)
  def force = (w += d, d = 0)
}
```

使用例を示す。なお、中間層で逆伝播を繰り返す際に、勾配は引数で渡される。その勾配を、専用の最終層で取り出す。

```scala
val params = Seq[DelaySGD]().toBuffer
val model3 = new Offset(5, new Sigmoid, ()=>params.append(new DelaySGD).last, new Output(1))
val model2 = new Offset(6, new Sigmoid, ()=>params.append(new DelaySGD).last, new Output(5, (x,e)=>e))
val model1 = new RNN(1, model2, model3)
```

時系列の具体的な例として、波形を学習させる。余弦波を受け取り、位相を遅らせて正弦波を予測する回帰問題である。

```scala
val x = Seq.tabulate(200)(n => Seq(0.5 * math.cos(0.02 * math.Pi * n) + 0.5))
val y = Seq.tabulate(200)(n => Seq(0.5 * math.sin(0.02 * math.Pi * n) + 0.5))
for(step <- 1 to 100000) model1.init -> x.zip(y).foreach(model1.bp(_,_) -> params.foreach(_.force))
```

Fig. 2.5に予測した波形と真の波形を示す。学習の初期段階では、歪な波形だが、学習が進むと、綺麗な正弦波に近付く。

![images/rnn.phase.png](/images/rnn.phase.png)

Fig. 2.5 sine curve prediction.

時系列の機械学習は、機械翻訳や映像生成など、発展が顕著な分野で、複雑な文脈構造を如何に捉えるかが課題である。

